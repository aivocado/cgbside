[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "KPMG Capstone Project",
    "section": "",
    "text": "이홍주, 이화정, 홍수민 으로 구성된 팀입니다."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CGBeta : CGINSIDE X KPMG Internship Team B",
    "section": "",
    "text": "프로젝트를 시작합니다!!!\n\n\n\n\n\n\n\n\n  \n\n\n\n\n1121_Sumin\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nSumin Hong\n\n\n\n\n\n\n  \n\n\n\n\n1121_Data Plan\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nHongJu Lee, Sumin Hong, Hwajeong Lee\n\n\n\n\n\n\n  \n\n\n\n\n1121_Hwajeong\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nHwajeong Lee\n\n\n\n\n\n\n  \n\n\n\n\n1109_HongJu\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nHongJu Lee\n\n\n\n\n\n\n  \n\n\n\n\nMarkDown 문법정리\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nLee hong ju\n\n\n\n\n\n\n  \n\n\n\n\n1109_Sumin\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nSumin Hong\n\n\n\n\n\n\n  \n\n\n\n\n1109_HongJu\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nHongJu Lee\n\n\n\n\n\n\n  \n\n\n\n\n1109_Hwajeong\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\nHwaJeong Lee\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 7, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/1109_sumin/index.html",
    "href": "posts/1109_sumin/index.html",
    "title": "1109_Sumin",
    "section": "",
    "text": "Open AI DevDay Keynote\n\n시행 일시: 2023.11.07\nOpen AI가 실시한 첫 개발자 컨퍼런스  \n\n\nOpenAI가 새롭게 발표한 기능 정리\n\nGPT Builder: 대화형 챗봇 개발 환경 \n\n챗봇 사용 목적 등을 설명해주고, 추가 자료(pdf, img 등) 입력하면 맞춤형 챗봇 개발 가능\nGPT Store 이용해 GPT Builder로 개발된 챗봇 공유 및 사용 가능\n\nGPT Store 사이트 링크: https://gptbuilderstore.com/ (11월 말 런칭 예정) \n\n\nAPI 개선\n\nAssistants API\n\nBlackbox 해소 위해 Code interpreter, 답 제공 시 사용된 주요 parameter 제공\n\nJSON 모드 추가\n\nJSON 타입으로 답변\n\nSEED 값 세팅 가능\nLog Probability 제공 \n\nGPT-4 Turbo 출시\n\nContext Length 증가\n\n32K → 128K\n\nMore user control\n\n답변의 형식 지정 가능\n여러 함수 한 번에 처리 가능\n외부 문서, 정보 학습시켜서 사용 가능\n\nNew modalities\n\nVision, Speech 가능\n\nData Update\n\n2023년 4월 정보까지 학습됨  \n\n\n\n\n\n느낀점..\n컨퍼런스 영상을 시청하며, 아래의 두 장면이 가장 기억에 남았다.  \n이제 인공지능이 음성명령만으로 중요한 정보를 담아 포스터를 만들어주고, 인공지능을 간편하게 커스텀하여 여행 가이드로 사용할 수 있는 시대가 되었다.\n이 장면들을 보며, 전 세계 사람들이 각자만의 아이디어로 커스텀한 인공지능을 공유한다면, 어떤 획기적인 인공지능 서비스가 탄생할지 11월 말이 기대가 되었다.\n그러나, 데이터 사이언티스트를 꿈꾸며 인공지능을 공부하는 학생으로서, 다소 우려가 되기도 하였다. 기존에는 인공지능을 활용해 서비스를 런칭할 때, 데이터를 수집하고, 정제하여, 코드로 fine-tuning하는 작업을 거쳐야했다. 그러나 이제는 코드 없이 음성명령만으로 간편하게 모델을 커스텀하는 것이 가능해졌다. 다시 말해, 인공지능 개발자나 데이터 사이언티스트가 아닌 일반인들도 단순히 인공지능 서비스를 소비하는 것을 넘어 인공지능을 활용할 수 있게 되었다. 이러한 상황에서 데이터 사이언티스트의 역할과 데이터 사이언티스트가 지녀야 할 소양이 무엇일지에 대한 고민을 해보게 되었다.\n추가적으로, 음성명령만으로 인공지능에게 입금을 하도록 하는 시연을 보며, 인공지능이 검색 엔진, 금융 시스템, 사내 데이터베이스 등과 결합된다면 훨씬 더 편리하고 효율적인 일상이 되겠다는 상상을 할 수 있었다. 다만, 이를 위해서는 개인정보 보호 문제, 저작권 문제 등이 먼저 해결되어야 하는 만큼, 기술적으로는 개인정보를 보호할 수 있는 방법에 대한 연구, 윤리적으로는 개인정보 및 저작물 활용에 대한 논의가 우선적으로 필요할 것으로 보인다."
  },
  {
    "objectID": "posts/1109_sumin/index.html#홍수민-블로그-테스트중입니다.",
    "href": "posts/1109_sumin/index.html#홍수민-블로그-테스트중입니다.",
    "title": "1109_Sumin",
    "section": "",
    "text": "세부 사항은 추후 추가해서 올리겠습니다."
  },
  {
    "objectID": "how_to_use_github.html",
    "href": "how_to_use_github.html",
    "title": "GitHub 협업 사용 방법 정리",
    "section": "",
    "text": "GitHub 협업 사용 방법 정리\n\n공유 레포지토리에서 fork로 내 고유 레포지토리로 불러오기\n내 레포지토리 URL을 내가 원하는 파일의 터미널로 들어가 [git clone URL주소] 입력\nvscode를 열고 clone한 파일 가져오기\n원하는 내용 수정\nvscode -&gt; 소스제어 -&gt; commit 메세지 입력 후 (commit + 동기화)\ngithub에서 변경사항 잘 저장 되었는지 확인\n변경사항 확인 후 이상 없으면 pull requests 진행\n관리자에게 merge 요청 하기\n\n\n(번외)\n\n기존 프로젝트 내용 가져오고 싶을땐 내 고유 레포지토리에 sync fork 진행 후 해당 폴더 경로로 들어가 [git pull] 후 작업 진행"
  },
  {
    "objectID": "posts/1109_hongju/index.html",
    "href": "posts/1109_hongju/index.html",
    "title": "1109_HongJu",
    "section": "",
    "text": "open AI 컨퍼런스 정리\n얼마 전 open ai에서 개발자들을 대상으로 devday 행사가 열렸다. 이에 유튜브로 해당 내용들을 확인하고 정리해 보고자 한다.\n\n\n\nAI 서비스의 핵심이 된 gpt의 사용자 현황\n\n\n이번 컨퍼런스의 핵심 내용은 gpts 서비스 제공인 것 같다. 앞서 발표자가 설명하는 gpt4 turbo의 변경사항들 모두 gpts 서비스 제공을 위한 개선 사항들이라고 할 수 있을 정도로 gpts 서비스 제공에 초점을 맞춘 업데이트라고 생각된다.\n\n\nGPT4 turbo 업데이트 사항\n\n수용 가능한 input의 크기 변화\n→ 기존의 input 크기 토큰 8,000개에서 토큰 128,000개로 input의 수용 가능한 크기를 대폭 향상시켜 더 큰 데이터를 학습 시키고 질문할 수 있게 바뀌었다. 이는 300페이지 짜리 문서도 학습 시킬 수 있다는 것으로 논문이나 책, 보고서 등의 학습도 가능해졌다.\napi 가격 절감\n→ 기존 gpt4와 비교했을 때 input은 3배, output은 2배의 비용을 절감시켜 일반 개발자들이 더 싼 비용으로 gpt의 ai 서비스 호출이 가능해졌다.\nJSON 형태로 output 지정 가능\n→ JSON 형태로 호출 지정을 가능하게 해줘 서버 및 다른 api와의 연동 또한 사용하기 쉬워졌다.\nDALL-E 3 모델 지원 및 text-음성 변환 기술 제공\n→ 사진의 캡션을 생성하거나, 실제 이미지를 상세히 분석하고, 그림이 포함된 문서를 읽는 등의 작업이 가능해졌다. 여기서 핵심은 api에서 DELL-E 3를 사용 가능해 졌기 때문에 자신이 생성한 앱에 이 기능을 api형태로 추가 가능해 졌다는 것이다.\n데이터 보안\n→ OpenAI API에 전달된 데이터와 파일은 모델을 훈련하는 데 사용되지 않으며, 개발자는 적절하다고 판단될 때 데이터를 삭제할 수 있어 기업의 데이터 유출 문제를 해결할 수 있다. 또한 데이터에 민감한 기업에게는 custom models을 제공해 다른 고객에게 제공되거나 공유되지 않고 다른 모델을 훈련하는 데에도 사용되지 않는다.\n\n\n\nGPTs 서비스\n\n\n\nassistant api 적용 화면\n\n\n\n\n\ncode interpreter 적용 화면\n\n\n앞서 설명한 gpt4 turbo 모델을 이용해서 최종적으로 open ai에서 할 목표는 gpts 서비스 제공이다. 대부분의 LLM모델의 단점이 모든 도메인을 아우르는 LLM모델을 만들기 어렵다는 점이다. 특정 도메인 분야에서 더 정확한 모델을 얻기 위해서는 각 도메인의 데이터를 이용해 사전 학습하는 과정을 거쳐야 하고 gpt에서는 학습을 위해 긴 프롬프팅이 필요하다. 이번 open ai 컨퍼런스의 gpts는 이 문제를 해결할 것으로 보인다. 앞서 설명했던 input 크기 향상, 다양한 형태의 데이터 api 전송 가능, 가격 인하로 인해 더 많은 개발자들이 도메인 맞춤형 gpt를 생성할 수 있게 되었고 이를 서비스해 수익까지 창출할 수 있게 되었다. 기존의 app store의 형태처럼 AI 서비스 역시 맞춤형 gpt 모델을 제공하면서 일반 사용자들이 보다 더 쉽게 ai 서비스 접근이 가능해진 것이다. 또한 모델의 사전 학습 단계에서도 gpt를 사용해 학습이 가능하기 때문에 일반 사용자들도 데이터만 있다면 ai 개발자가 되어 자신만의 맞춤형 모델을 생성하고 서비스 가능해졌다. 실제로 open ai에서 시연했던 assistant api를 보면 이것이 구현 가능함을 보여줬다. 이로 인해 AI에 대한 일반인들의 접근성이 좋아져 더 활발한 AI 생태계 구축을 목표로 하는 것 같다.\n\n\nGPTs를 보며 든 나의 생각\ngpts를 보면서 나는 두 가지 사항을 생각해 보았다.\n첫 번째로 산업의 개편이다. gpts가 상용화된다면 AI서비스를 개인 맞춤형으로 사용 가능해질 것이다. assistant api와 같은 서비스가 활성화된다면 기존의 AI모델을 사용 및 개발하기 위한 사전 지식의 역치가 많이 낮아질 것이다. 따라서 개발자나 컨설턴트 등의 직종이 심각하게 위협을 받을 것으로 예상된다.\n두 번째로는 정보 보안이 더욱 중요해질 것으로 보인다. 컨퍼런스에서는 기업의 데이터를 학습하지 않는다고 했지만 이는 open ai에서 주장하는 바로 실제로 데이터를 학습하는지 여부는 알 수 없다. 삼성, KT과 같은 대기업들에서 gpt를 사용하지 않고 자체적인 LLM모델을 만들어 사용하려는 이유도 여기에 있다. 따라서 데이터를 암호화해서 학습시키고 결과를 도출해 내는 동형 암호와 같은 암호 기술들의 발전이 산업에서의 AI 활성화에 있어 필수 조건이라고 생각된다."
  },
  {
    "objectID": "posts/1109_hongju/index.html#이홍주-블로그-테스트중입니다.",
    "href": "posts/1109_hongju/index.html#이홍주-블로그-테스트중입니다.",
    "title": "1109_HongJu",
    "section": "",
    "text": "세부 사항은 추후 추가해서 올리겠습니다."
  },
  {
    "objectID": "posts/1109_hwajeong/index.html",
    "href": "posts/1109_hwajeong/index.html",
    "title": "1109_Hwajeong",
    "section": "",
    "text": "지난 1년간, OpenAI는 ChatGPT, GPT-4, DALL•E 3(이미지 모델) 등의 모델을 출시하였고, 약 2백만 개발자와 92% 이상의 포춘 500 대기업이 OpenAI 제품을 사용하고 있다고 한다.\n\nChatGPT는 주간 활성 사용자가 무려 약 1억 명에 이른다고 한다. 이를 통해 OpenAI는 현재 세계에서 가장 선진적이고 널리 사용되는 AI 플랫폼임을 입증한 것 같다.\n\n\n\n\nKeynote의 하이라이트는 다양한 사용자 요구를 처리하는 혁신적인 모델인 GPT-4 Turbo의 소개였다. 여섯 가지 주요 개선 사항을 반영하였는데 내용은 다음과 같다.\n\nContext length : 기존 8k context에서 16배 긴 128k context 지원. (128,000 토큰, 표준 서적의 300페이지에 달함)\nMore control : Json Load 기능, Logprobs 기능, 재현 가능한 출력이라는 새로운 기능을 도입(시드 매개변수를 전달하면 모델이 일관된 출력을 반환한다.)\nBetter knowledge : 검색 기능을 도입하여 외부 지식 통합가능, 지식 cutoff도 2023.4월로 업데이트 됨.\nNew modalities : DALL•E 3, GPT-4 Turbo, 그리고 새로운 TTS 모델이 모두 API에 도입됨. *오픈 소스 음성 인식 모델인 Whisper V3의 다음 버전 출시 예정이며 이 또한 곧 API에 도입될 예정.\nCostomization : GPT-3.5 16k fine-tuning 가능, GPT-4 fine-tuning 일부 사용자에게 access 허용, custom model 출시(기업 고객용)\nHigher rate limit : 토큰당 분당 속도를 두 배로 늘릴 예정. API 계정 설정에서 추가적인 속도 제한과 할당량을 변경할 수 있게 됨.\n\n\n\n\n\nOpenai devday에서, 발표자들이 ’저작권’과 ’보안’에 대한 언급을 자주 하는 것을 볼 수 있다.\n\n기업 고객들을 더 끌어들이기 위해서는 걸림돌인 ‘저작권 및 보안’ 문제를 언급하지 않을 수 없는데 이 문제에서 벗어나기 위한 노력의 일환으로 ‘Copyright Shield’ 서비스 또한 새롭게 선보였다.\n\n저작권 침해에 대한 법적 주장이 제기될 경우, OpenAI가 고객을 대신하여 법적 대응을 지원하고 관련 비용을 부담한다는 내용이었다. ChatGPT Enterprise와 API 모두에 적용이 될 예정이다.\n\n다시한번, OpenAI는 API나 ChatGPT Enterprise에서의 데이터로 훈련하지 않는다는 점을 강조하였다.\n\n+추가적으로 가장 중요한 2가지 개선사항을 발표하였다.\n\n\n\n\nGPT-4 Turbo : 천 개의 프롬프트 토큰당 1센트, 천 개의 완료 토큰당 0.03달러 (프롬프트 토큰에 대해 3배, 완료 토큰에 대해 2배의 비율로 GPT-4보다 저렴, 3.75% 이상 더 저렴)\nGPT-3.5 Turbo 16k : input 토큰은 3배, ouput 토큰은 2배 더 저렴\nGPT-4 Turbo 속도 개선 예정\n\n여기까지, 모델 자체에 대한 개선사항이었다.\n\n\n\n다음으로는 Microsoft의 CEO인 Satya Nadella가 특별 게스트로 나왔는데, OpenAI와의 파트너십에 대해 강조하였다.\n\nAzure를 기반으로 최고의 시스템을 구축하고 개발자들에게 제공하여 최고의 모델을 만들 수 있도록 지원하겠다고 밝혔다. 또한, 개발자로서 자체 제품을 개발 중이며, OpenAI의 API를 활용하여 GitHub Copilot과 같은 제품을 만들 계획이라고 한다. (현재 Copilot 기능에 gpt api를 도입하는 것 같다.) Azure 마켓플레이스를 통해 제품을 신속히 출시하고 시장에 내놓을 예정이며 마지막으로, Microsoft와 OpenAI의 공통 목표는 인공지능의 이점을 모든 사람에게 전달하는 것이라며 마무리를 하였다.\n\n+또 다른 ‘작은’ 개선사항 하나 더!\n모델 picker가 사라졌다! (드롭다운 박스 없어짐) ChatGPT가 알아서, 언제 어떤 것을 사용해야 하는지 자동으로 처리해 준다고 한다.\n\n\n\n\nOpenAI는 AI의 안전성 문제를 해결하는 가장 효과적인 방법은 점진적이고 반복적인 배포라고 강조하고 있다. 이에 그 첫 단계로 GPTs라는 새로운 기술을 소개했다.\nGPTs는 특정 목적을 위해 구성된 ChatGPT의 맞춤형 버전이다.\n여기서도 ’보안’에 대한 언급을 한다. 모든 작업을 수행하기 전 ’allow’를 통해 사용자의 허가를 받게 끔 설정되어 있다.\n코딩 없이, 자연어로만으로 GPT를 만드는 데모를 보여준다.\n\n\n\n\n위에서 소개한 다양한 맞춤형 GPTs들을 생성하고 공유할 수 있는 마켓플레이스인 GPT Store가 출시예정이다.\n+사용자 수에 따른 수익 창출도 가능하다고 하니, 다양한 사람들의 다양한 GPT들이 공유될 수 있는 장을 마련한 것 같다. 어떤 GPT들이 나올지 기대가 된다.\n\n\n\n개인적으로는 GPT API를 통해서 개발 경험이 없어서 와닿지는 않았던 부분이지만, 개발자들이 API를 활용하여 더욱 효율적으로 개발할 수 있도록 개선된 부분인 것 같다.\n\nGPT API를 활용하여 에이전트 형태의 경험을 제공하는 앱이 이미 개발되고 있는데, 과거에는 이를 개발하는 것이 어렵고 시간이 많이 걸렸다.하지만 새롭게 출시된 Assistants API를 통해 이를 더 쉽고 편리하게 할 수 있다.\nAssistants API에는 Threading, Python 인터프리터, Function calling 등 다양한 기능이 포함되어 있다.\n음성인식으로 만든 Assistant API를 통해서 Devday 참석자들에게 토큰 500달러를 지급하는 데모를 보여주었다!\n\n\n\n\n\nOpenAI의 Devday를 통해 다양한 기능들이 소개되었다. GPT-4 Turbo는 기존의 GPT-3.5와 비교했을 때 더욱 강력한 기능들을 가지고 있었다. 특히 GPTs, GPT store가 출시됨에 따라 어떤 다양한 커스텀된 GPT들이 나올지 기대가 된다.\nAI가 모든 것에 통합되면서, 마치 스마트폰이 우리의 삶에 깊숙이 녹아든 것처럼, ChatGPT 없이는 생활이 불가능한 시대가 올 것 같다.\n인상깊었던 부분은, OpenAI가 많은 사람들이 인공지능 기술을 접근 가능하게 하기위해 노력하고 있는 부분이었다. OpensAI는 AI기술을 사용할 때 개인의 권한 및 효용성을 최고로 살리는 것이 중요하다는 점을 끊임없이 강조한다. 이를 통해 개인들이 더 나은 도구를 활용하여 세상을 변화시킬 수 있는 기회를 얻을 수 있을 것 같다."
  },
  {
    "objectID": "posts/1109_hwajeong/index.html#이화정-블로그-테스트중입니다.",
    "href": "posts/1109_hwajeong/index.html#이화정-블로그-테스트중입니다.",
    "title": "1109_Hwajeong",
    "section": "",
    "text": "세부 사항은 추후 추가해서 올리겠습니다."
  },
  {
    "objectID": "posts/how_to_use_markdown/index.html",
    "href": "posts/how_to_use_markdown/index.html",
    "title": "MarkDown 문법정리",
    "section": "",
    "text": "이건 인용하는데 사용하는 코드입니다.\n\n\n\n\ngit 공부\n프로젝트 수정\n\n\n\n\n\n작대기\n\n점\n\n더하기\n\n\n\n\n\n\npython 코드\ndef greet(name):\n    print(f\"Hello, {name}!\")\ndef hello():\n    print(\"hello world\")\n코드 종료\n\n\n\n\n구글 링크 추가 구글 링크\n\n\n\n\n\n\n\nAlt text"
  },
  {
    "objectID": "posts/how_to_use_markdown/index.html#마크다운-문법-정리입니다.",
    "href": "posts/how_to_use_markdown/index.html#마크다운-문법-정리입니다.",
    "title": "MarkDown 문법정리",
    "section": "",
    "text": "이건 인용하는데 사용하는 코드입니다.\n\n\n\n\ngit 공부\n프로젝트 수정\n\n\n\n\n\n작대기\n\n점\n\n더하기\n\n\n\n\n\n\npython 코드\ndef greet(name):\n    print(f\"Hello, {name}!\")\ndef hello():\n    print(\"hello world\")\n코드 종료\n\n\n\n\n구글 링크 추가 구글 링크\n\n\n\n\n\n\n\nAlt text"
  },
  {
    "objectID": "about.html#이홍주",
    "href": "about.html#이홍주",
    "title": "KPMG 캡스톤",
    "section": "이홍주",
    "text": "이홍주"
  },
  {
    "objectID": "ihongju.html",
    "href": "ihongju.html",
    "title": "CGINSIDE_TEAM B",
    "section": "",
    "text": "여기에 이홍주의 추가 정보 및 프로필 내용을 작성하세요."
  },
  {
    "objectID": "posts/1109_hwajeong/index.html#openai-devday-개막-keynote",
    "href": "posts/1109_hwajeong/index.html#openai-devday-개막-keynote",
    "title": "1109_Hwajeong",
    "section": "",
    "text": "지난 1년간, OpenAI는 ChatGPT, GPT-4, DALL•E 3(이미지 모델) 등의 모델을 출시하였고, 약 2백만 개발자와 92% 이상의 포춘 500 대기업이 OpenAI 제품을 사용하고 있다고 한다.  ChatGPT는 주간 활성 사용자가 무려 약 1억 명에 이른다고 한다. 이를 통해 OpenAI는 현재 세계에서 가장 선진적이고 널리 사용되는 AI 플랫폼임을 입증한 것 같다.\n\n\n\nKeynote의 하이라이트는 다양한 사용자 요구를 처리하는 혁신적인 모델인 GPT-4 Turbo의 소개였다. 여섯 가지 주요 개선 사항을 반영하였는데 내용은 다음과 같다.\n\nContext length : 기존 8k context에서 16배 긴 128k context 지원. (128,000 토큰, 표준 서적의 300페이지에 달함)\nMore control : Json Load 기능, Logprobs 기능, 재현 가능한 출력이라는 새로운 기능을 도입(시드 매개변수를 전달하면 모델이 일관된 출력을 반환한다.)\nBetter knowledge : 검색 기능을 도입하여 외부 지식 통합가능, 지식 cutoff도 2023.4월로 업데이트 됨.\nNew modalities : DALL•E 3, GPT-4 Turbo, 그리고 새로운 TTS 모델이 모두 API에 도입됨. *오픈 소스 음성 인식 모델인 Whisper V3의 다음 버전 출시 예정이며 이 또한 곧 API에 도입될 예정.\nCostomization : GPT-3.5 16k fine-tuning 가능, GPT-4 fine-tuning 일부 사용자에게 access 허용, custom model 출시(기업 고객용)\nHigher rate limit : 토큰당 분당 속도를 두 배로 늘릴 예정. API 계정 설정에서 추가적인 속도 제한과 할당량을 변경할 수 있게 됨.\n\n\n\n\nOpenai devday에서, 발표자들이 ‘저작권’과 ‘보안’에 대한 언급을 자주 하는 것을 볼 수 있다. 기업 고객들을 더 끌어들이기 위해서는 걸림돌인 ’저작권 및 보안’ 문제를 언급하지 않을 수 없는데 이 문제에서 벗어나기 위한 노력의 일환으로 ‘Copyright Shield’ 서비스 또한 새롭게 선보였다. 저작권 침해에 대한 법적 주장이 제기될 경우, OpenAI가 고객을 대신하여 법적 대응을 지원하고 관련 비용을 부담한다는 내용이었다. ChatGPT Enterprise와 API 모두에 적용이 될 예정이다.  다시한번, OpenAI는 API나 ChatGPT Enterprise에서의 데이터로 훈련하지 않는다는 점을 강조하였다.\n+추가적으로 가장 중요한 2가지 개선사항을 발표하였다.\n\n\n\n\nGPT-4 Turbo : 천 개의 프롬프트 토큰당 1센트, 천 개의 완료 토큰당 0.03달러 (프롬프트 토큰에 대해 3배, 완료 토큰에 대해 2배의 비율로 GPT-4보다 저렴, 3.75% 이상 더 저렴)\nGPT-3.5 Turbo 16k : input 토큰은 3배, ouput 토큰은 2배 더 저렴\nGPT-4 Turbo 속도 개선 예정\n\n여기까지, 모델 자체에 대한 개선사항이었다.\n\n\n\n다음으로는 Microsoft의 CEO인 Satya Nadella가 특별 게스트로 나왔는데, OpenAI와의 파트너십에 대해 강조하였다.\n\nAzure를 기반으로 최고의 시스템을 구축하고 개발자들에게 제공하여 최고의 모델을 만들 수 있도록 지원하겠다고 밝혔다. 또한, 개발자로서 자체 제품을 개발 중이며, OpenAI의 API를 활용하여 GitHub Copilot과 같은 제품을 만들 계획이라고 한다. (현재 Copilot 기능에 gpt api를 도입하는 것 같다.) Azure 마켓플레이스를 통해 제품을 신속히 출시하고 시장에 내놓을 예정이며 마지막으로, Microsoft와 OpenAI의 공통 목표는 인공지능의 이점을 모든 사람에게 전달하는 것이라며 마무리를 하였다.\n\n+또 다른 ‘작은’ 개선사항 하나 더!\n모델 picker가 사라졌다! (드롭다운 박스 없어짐) ChatGPT가 알아서, 언제 어떤 것을 사용해야 하는지 자동으로 처리해 준다고 한다.\n\n\n\n\nOpenAI는 AI의 안전성 문제를 해결하는 가장 효과적인 방법은 점진적이고 반복적인 배포라고 강조하고 있다. 이에 그 첫 단계로 GPTs라는 새로운 기술을 소개했다.\nGPTs는 특정 목적을 위해 구성된 ChatGPT의 맞춤형 버전이다.\n여기서도 ’보안’에 대한 언급을 한다. 모든 작업을 수행하기 전 ’allow’를 통해 사용자의 허가를 받게 끔 설정되어 있다.\n코딩 없이, 자연어로만으로 GPT를 만드는 데모를 보여준다.\n\n\n\n\n위에서 소개한 다양한 맞춤형 GPTs들을 생성하고 공유할 수 있는 마켓플레이스인 GPT Store가 출시예정이다.\n+사용자 수에 따른 수익 창출도 가능하다고 하니, 다양한 사람들의 다양한 GPT들이 공유될 수 있는 장을 마련한 것 같다. 어떤 GPT들이 나올지 기대가 된다.\n\n\n\n개인적으로는 GPT API를 통해서 개발 경험이 없어서 와닿지는 않았던 부분이지만, 개발자들이 더욱 효율적으로 개발할 수 있도록 개선된 부분인 것 같다. - GPT API를 활용하여 에이전트 형태의 경험을 제공하는 앱이 이미 개발되고 있는데, 과거에는 이를 개발하는 것이 어렵고 시간이 많이 걸렸다. - 하지만 새롭게 출시된 Assistants API를 통해 이를 더 쉽고 편리하게 할 수 있다. - Assistants API에는 대화 기록, Python 인터프리터, 기능 호출 등 다양한 기능이 포함되어 있다. - 음성인식으로 만든 Assistant API를 통해서 Devday 참석자들에게 토큰 500달러를 지급하는 데모를 보여주었다!\n\n\n\n\nOpenAI의 Devday를 통해 다양한 기능들이 소개되었다. GPT-4 Turbo는 기존의 GPT-3.5와 비교했을 때 더욱 강력한 기능들을 가지고 있었다. 특히 GPTs, GPT store가 출시됨에 따라 얼마나 다양한 커스텀된 GPT들이 나올지 기대가 된다.\nAI가 모든 것에 통합되면서, 마치 스마트폰이 우리의 삶에 깊숙이 녹아든 것처럼, ChatGPT 없이는 생활이 불가능한 시대가 올 것 같다.\n인상깊었던 부분은, OpenAI가 많은 사람들이 인공지능 기술을 접근 가능하게 하기위해 노력하고 있는 부분이었다. OpensAI는 AI기술을 사용할 때 개인의 권한 및 효용성을 최고로 살리는 것이 중요하다는 점을 끊임없이 강조한다. 이를 통해 개인들이 더 나은 도구를 활용하여 세상을 변화시킬 수 있는 기회를 얻을 수 있을 것 같다."
  },
  {
    "objectID": "posts/1109_hwajeong/index.html#월-말-gpt-store-공개-예정",
    "href": "posts/1109_hwajeong/index.html#월-말-gpt-store-공개-예정",
    "title": "1109_Hwajeong",
    "section": "11월 말, GPT Store 공개 예정",
    "text": "11월 말, GPT Store 공개 예정\nOpenAI는 GPT Store를 공개했습니다. 이는 개발자들이 다양한 애플리케이션에 대해 맞춤형 GPT를 생성하고 공유할 수 있는 마켓플레이스입니다. GPT Store는 자연어 프로그래밍을 강조하며, 사용자가 대화식으로 상호작용하여 GPT를 프로그래밍할 수 있게 합니다.\n수익 분배도 가능함\n\nassistant API에서의 새로운 모달리티와 개선 사항(개발자들을 위한)\n앱 내 특수 보조자, 문서 파싱 기능, 그리고 동적 코드 생성 및 실행을 가능하게 하는 Code Interpreter가 포함되어있다.\n\nGPT API를 활용하여 에이전트 형태의 경험을 제공하는 앱이 이미 개발되고 있는데, 과거에는 이를 개발하는 것이 어렵고 시간이 많이 걸렸다.\n하지만 오늘날, 새롭게 출시된 Assistants API를 통해 이를 더 쉽고 편리하게 할 수 있다.\nAssistants API에는 대화 기록, Python 인터프리터, 기능 호출 등 다양한 기능이 포함되어 있다.\n또한, 개발자 경험도 개선되었으며, 이를 위해 새로운 모달리티가 도입되었다\n\n데모(음성인식으로 만든 assistant api를 통해서 500달러 지급)\n\n\n\n\n결론\n\nAI 기술을 사용할 때 개인의 권한 및 효용성을 최고로 살리는 것이 가장 중요하다고 본다. 이를 통해 인류는 세계사에서 결코 본 적 없는 차원의 혁신을 이룰 수 있을 것이다.\n모든 것이 AI로 접목됨에 따라, 우리는 모두가 수요에 따라 상황에 맞추어 초능력을 발휘할 수 있게 될 것이다.\nOpenAI 팀은 이러한 기술이 보다 혁신적이고 발전된 모습으로 발표될 것을 약속하며, 우리가 함께 만들어갈 길고 놀라운 미래를 기대한다.\nopenai devday를 보면, 중간중간 발표자들이 ’저작권’과 ’보안’에 대한 언급을 자주 하는 것을 볼 수 있다.\n\n\n우리는 ’저작권 보호’를 도입하고 있습니다. 저작권 보호란 저희가 고객들을 지원하고, 저작권 침해에 대한 법적 주장에 직면했을 때 발생하는 비용을 지불하겠다는 것을 의미하며, 이는 ChatGPT Enterprise와 API 모두에 적용됩니다. 확실히 말하자면, 이것은 사람들에게 우리가 API나 ChatGPT Enterprise에서의 데이터로 훈련하지 않는다는 것을 상기시키는 좋은 시기입니다."
  },
  {
    "objectID": "posts/1109_hwajeong/index.html#openai-devday-opening-keynote-리뷰",
    "href": "posts/1109_hwajeong/index.html#openai-devday-opening-keynote-리뷰",
    "title": "1109_Hwajeong",
    "section": "",
    "text": "지난 1년간, OpenAI는 ChatGPT, GPT-4, DALL•E 3(이미지 모델) 등의 모델을 출시하였고, 약 2백만 개발자와 92% 이상의 포춘 500 대기업이 OpenAI 제품을 사용하고 있다고 한다.\n\nChatGPT는 주간 활성 사용자가 무려 약 1억 명에 이른다고 한다. 이를 통해 OpenAI는 현재 세계에서 가장 선진적이고 널리 사용되는 AI 플랫폼임을 입증한 것 같다.\n\n\n\n\nKeynote의 하이라이트는 다양한 사용자 요구를 처리하는 혁신적인 모델인 GPT-4 Turbo의 소개였다. 여섯 가지 주요 개선 사항을 반영하였는데 내용은 다음과 같다.\n\nContext length : 기존 8k context에서 16배 긴 128k context 지원. (128,000 토큰, 표준 서적의 300페이지에 달함)\nMore control : Json Load 기능, Logprobs 기능, 재현 가능한 출력이라는 새로운 기능을 도입(시드 매개변수를 전달하면 모델이 일관된 출력을 반환한다.)\nBetter knowledge : 검색 기능을 도입하여 외부 지식 통합가능, 지식 cutoff도 2023.4월로 업데이트 됨.\nNew modalities : DALL•E 3, GPT-4 Turbo, 그리고 새로운 TTS 모델이 모두 API에 도입됨. *오픈 소스 음성 인식 모델인 Whisper V3의 다음 버전 출시 예정이며 이 또한 곧 API에 도입될 예정.\nCostomization : GPT-3.5 16k fine-tuning 가능, GPT-4 fine-tuning 일부 사용자에게 access 허용, custom model 출시(기업 고객용)\nHigher rate limit : 토큰당 분당 속도를 두 배로 늘릴 예정. API 계정 설정에서 추가적인 속도 제한과 할당량을 변경할 수 있게 됨.\n\n\n\n\n\nOpenai devday에서, 발표자들이 ’저작권’과 ’보안’에 대한 언급을 자주 하는 것을 볼 수 있다.\n\n기업 고객들을 더 끌어들이기 위해서는 걸림돌인 ‘저작권 및 보안’ 문제를 언급하지 않을 수 없는데 이 문제에서 벗어나기 위한 노력의 일환으로 ‘Copyright Shield’ 서비스 또한 새롭게 선보였다.\n\n저작권 침해에 대한 법적 주장이 제기될 경우, OpenAI가 고객을 대신하여 법적 대응을 지원하고 관련 비용을 부담한다는 내용이었다. ChatGPT Enterprise와 API 모두에 적용이 될 예정이다.\n\n다시한번, OpenAI는 API나 ChatGPT Enterprise에서의 데이터로 훈련하지 않는다는 점을 강조하였다.\n\n+추가적으로 가장 중요한 2가지 개선사항을 발표하였다.\n\n\n\n\nGPT-4 Turbo : 천 개의 프롬프트 토큰당 1센트, 천 개의 완료 토큰당 0.03달러 (프롬프트 토큰에 대해 3배, 완료 토큰에 대해 2배의 비율로 GPT-4보다 저렴, 3.75% 이상 더 저렴)\nGPT-3.5 Turbo 16k : input 토큰은 3배, ouput 토큰은 2배 더 저렴\nGPT-4 Turbo 속도 개선 예정\n\n여기까지, 모델 자체에 대한 개선사항이었다.\n\n\n\n다음으로는 Microsoft의 CEO인 Satya Nadella가 특별 게스트로 나왔는데, OpenAI와의 파트너십에 대해 강조하였다.\n\nAzure를 기반으로 최고의 시스템을 구축하고 개발자들에게 제공하여 최고의 모델을 만들 수 있도록 지원하겠다고 밝혔다. 또한, 개발자로서 자체 제품을 개발 중이며, OpenAI의 API를 활용하여 GitHub Copilot과 같은 제품을 만들 계획이라고 한다. (현재 Copilot 기능에 gpt api를 도입하는 것 같다.) Azure 마켓플레이스를 통해 제품을 신속히 출시하고 시장에 내놓을 예정이며 마지막으로, Microsoft와 OpenAI의 공통 목표는 인공지능의 이점을 모든 사람에게 전달하는 것이라며 마무리를 하였다.\n\n+또 다른 ‘작은’ 개선사항 하나 더!\n모델 picker가 사라졌다! (드롭다운 박스 없어짐) ChatGPT가 알아서, 언제 어떤 것을 사용해야 하는지 자동으로 처리해 준다고 한다.\n\n\n\n\nOpenAI는 AI의 안전성 문제를 해결하는 가장 효과적인 방법은 점진적이고 반복적인 배포라고 강조하고 있다. 이에 그 첫 단계로 GPTs라는 새로운 기술을 소개했다.\nGPTs는 특정 목적을 위해 구성된 ChatGPT의 맞춤형 버전이다.\n여기서도 ’보안’에 대한 언급을 한다. 모든 작업을 수행하기 전 ’allow’를 통해 사용자의 허가를 받게 끔 설정되어 있다.\n코딩 없이, 자연어로만으로 GPT를 만드는 데모를 보여준다.\n\n\n\n\n위에서 소개한 다양한 맞춤형 GPTs들을 생성하고 공유할 수 있는 마켓플레이스인 GPT Store가 출시예정이다.\n+사용자 수에 따른 수익 창출도 가능하다고 하니, 다양한 사람들의 다양한 GPT들이 공유될 수 있는 장을 마련한 것 같다. 어떤 GPT들이 나올지 기대가 된다.\n\n\n\n개인적으로는 GPT API를 통해서 개발 경험이 없어서 와닿지는 않았던 부분이지만, 개발자들이 API를 활용하여 더욱 효율적으로 개발할 수 있도록 개선된 부분인 것 같다.\n\nGPT API를 활용하여 에이전트 형태의 경험을 제공하는 앱이 이미 개발되고 있는데, 과거에는 이를 개발하는 것이 어렵고 시간이 많이 걸렸다.하지만 새롭게 출시된 Assistants API를 통해 이를 더 쉽고 편리하게 할 수 있다.\nAssistants API에는 Threading, Python 인터프리터, Function calling 등 다양한 기능이 포함되어 있다.\n음성인식으로 만든 Assistant API를 통해서 Devday 참석자들에게 토큰 500달러를 지급하는 데모를 보여주었다!\n\n\n\n\n\nOpenAI의 Devday를 통해 다양한 기능들이 소개되었다. GPT-4 Turbo는 기존의 GPT-3.5와 비교했을 때 더욱 강력한 기능들을 가지고 있었다. 특히 GPTs, GPT store가 출시됨에 따라 어떤 다양한 커스텀된 GPT들이 나올지 기대가 된다.\nAI가 모든 것에 통합되면서, 마치 스마트폰이 우리의 삶에 깊숙이 녹아든 것처럼, ChatGPT 없이는 생활이 불가능한 시대가 올 것 같다.\n인상깊었던 부분은, OpenAI가 많은 사람들이 인공지능 기술을 접근 가능하게 하기위해 노력하고 있는 부분이었다. OpensAI는 AI기술을 사용할 때 개인의 권한 및 효용성을 최고로 살리는 것이 중요하다는 점을 끊임없이 강조한다. 이를 통해 개인들이 더 나은 도구를 활용하여 세상을 변화시킬 수 있는 기회를 얻을 수 있을 것 같다."
  },
  {
    "objectID": "about.html#team-members",
    "href": "about.html#team-members",
    "title": "CGBeta",
    "section": "Team Members",
    "text": "Team Members\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이홍주\n이화정\n홍수민\n\n\n데이터 사이언티스트\n데이터 사이언티스트\n데이터 사이언티스트"
  },
  {
    "objectID": "about.html#kpmg-capstone-project",
    "href": "about.html#kpmg-capstone-project",
    "title": "KPMG Capstone Project",
    "section": "KPMG Capstone Project",
    "text": "KPMG Capstone Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n이홍주\n이화정\n홍수민\n\n\n데이터 사이언티스트\n데이터 사이언티스트\n데이터 사이언티스트"
  },
  {
    "objectID": "posts/1120_hongju/index.html#도메인-지식의-중요성",
    "href": "posts/1120_hongju/index.html#도메인-지식의-중요성",
    "title": "1109_HongJu",
    "section": "1. 도메인 지식의 중요성",
    "text": "1. 도메인 지식의 중요성\n→ 법률 관련 지식이 거의 없다 보니 전달받은 데이터를 확인했을 때 법조문의 분류 체계 및 소관 부서들의 역할들을 잘 몰라 데이터의 구조를 이해하는데 시간이 오래 걸렸다.\nex) 법령 이름과 조문 제목의 차이들에 대해 잘 몰라서 이를 어떻게 구분하는지, provision_key_no 에 들어가 있는 조, 항, 호, 목을 구분하는 방법 등에서 어려움을 겪었다.\n\n\n\n법조문 분류 표"
  },
  {
    "objectID": "posts/1120_hongju/index.html#문자열-데이터-전처리의-어려움",
    "href": "posts/1120_hongju/index.html#문자열-데이터-전처리의-어려움",
    "title": "1109_HongJu",
    "section": "2. 문자열 데이터 전처리의 어려움",
    "text": "2. 문자열 데이터 전처리의 어려움\n→ 데이터를 열어보니 총 60,964개의 법조문이 존재했다. 조, 항을 제외한 나머지 열들이 모두 str 타입이기 때문에 문자열을 어떻게 전처리 할지가 중요하다고 생각했다.\n내가 생각한 전처리 방식은 다음과 같았다. 먼저 열 별로 unique 처리해서 줄일 수 있는 라벨로 변경할 수 있는 열이 무엇이 있는지 확인해 봤다. 처리 결과는 다음과 같았다.\nministrys = main_df['소관부처'].unique()\nlaws = main_df['법령'].unique()\nnames = main_df['조문제목'].unique()\nwork_name = main_df['사무명'].unique()\ntypes = main_df['사무유형'].unique()\nnote = main_df['비고(사무수행주체, 권한위임위탁 근거규정 등 입력)'].unique()\n\nprint(f\"소관부서: {len(ministrys)}개\")\nprint(f\"법령: {len(laws)}개\")\nprint(f\"조문제목: {len(names)}개\")\nprint(f\"사무명: {len(work_name)}개\")\nprint(f\"사무유형: {len(types)}개\")\nprint(f\"비고: {len(note)}개\")\n(결과)\n\n띄어쓰기 처리전 변수별 unique 개수\n\n띄어쓰기 처리 후 변수별 unique 개수\n확인 결과가 이상해서 데이터를 유심히 살펴봤다. 내가 전달받은 사무 유형은 총 16가지였는데 33가지가 나왔기 때문에 더 유심히 데이터를 살펴본 결과 띄어쓰기와 오탈자 등의 문제가 있었다. 따라서 이러한 것들을 EDA 과정에서 찾아내 적절한 범주로 묶고 라벨 인코딩을 진행해야겠다는 생각을 하게 되었다. 추가적으로 사뭇 명은 겹치는 것이 별로 없기 때문에 이 변수를 군집화할지, 자연어 생성 모델을 이용할지 고민해 봐야겠다는 생각을 했다."
  },
  {
    "objectID": "posts/1120_sumin/index.html",
    "href": "posts/1120_sumin/index.html",
    "title": "1121_Sumin",
    "section": "",
    "text": "1121_홍수민 데이터 분석 소감\n\n이번주 진행사항\n\nEDA\n향후 진행방향 논의 \n\n\n\n1. 데이터를 처음 열어본 소감..\n 처음 데이터를 받아 파일을 열려고 했는데, 위와 같이 파일이 열리지 않는 오류가 발생했다. 알고보니 eof(end of file)가 잘 마무리되지 않아 발생한 문제였다. 그동안 수업에서 데이터를 다룰 때는 한 번도 파일이 열리는 과정에서 에러가 나지 않았어서, 이 때부터 가공되지 않은 raw data의 위력을 실감할 수 있었다.\n엑셀 파일의 경우, 코드북과 실제 데이터셋 안에 있는 column, 데이터 형식이 일치하지 않았다. csv파일의 경우, provision_key가 왜 4자리~6자리로 구성되어 있는지, clause_key에서 각 숫자들이 의미하는 바가 무엇인지 유추해내는 것이 쉽지 않았다.  위와 같이 각 데이터의 의미를 파악을 하는 데에만 상당한 시간이 소요되었다. 법 체계와 사무 추출 작업에 대한 도메인 지식이 있었다면 이 작업을 조금 더 빨리 끝낼 수 있었겠다는 생각이 들었다. 데이터 분석에서 도메인 지식의 중요성을 느꼈던 것 같다. 이에 이동할 때 틈틈히 사무총조사 보고서 파일을 읽으며 부족한 도메인 지식을 채우려 노력하였다. 그래도 혼자 파악했다면 정말 오랜 시간이 걸렸을 것 같은데, 임원들과 공유 notion에 함께 알게 된 내용을 정리하고 공유하며 프로젝트를 진행하여서 더 많은 내용을 더 짧은 시간에 효율적으로 습득할 수 있었다.  \n\n\n2.주된 느낀 점\n이번 주 작업 중 가장 주되게 느낀 것은 EDA와 전처리의 중요성이다. 평소 데이터 전처리가 데이터 분석 과정에서 약 70%를 차지한다는 말을 들어보았으나, 사실 이전에는 크게 이 말을 실감하지 못하였다. Kaggle 등의 데이터를 다루어보았을 때는 전처리를 “모델 성능을 조금 더 높여줄 수도 있는 작업”정도로 생각하였다. 그러나, 이번 기회에 전처리 되지 않은 raw data로는 모델링을 시도할 수 없는 것을 깨달았다. 전처리를 “모델링을 하기 위해 꼭 필요한 작업”으로 인식이 바뀌게 되었다.  \n\n\n3. 배운 내용 활용 방안\n[전처리 방안]  - 법 조문에서 주요한 부분(주어, 서술어 등)을 추출한 후, 이를 따로 column으로 활용해도 좋겠다는 생각을 하였다.  - 국가와 지방자치단체에 소속된 기관을 위계를 표현할 수 있는 tree형태로 정리할 수 있다면, 훨씬 더 정확도 높은 분석을 수행할 수 있을 것 같았다. 이를 머신러닝과 딥러닝이 자체적으로 파악할 수 있을지, 아니면 시간이 많이 걸리더라도 사람이 수작업으로 정리한 후에 학습을 시키면 좋을지 향후 분석을 진행하며 판단해야할 것 같다. \n[모델링 방안]  - 성능이 높다고 평가된 XGBoost, LightGBM 등의 머신러닝 기법을 사용  - 성능이 높다고 평가된 최신 자연어처리 딥러닝 기법을 사용  - 실무에서 생성형 AI의 활용에 대한 관심이 많아지고 있는 것 같다. 이에 생성형 AI를 본 task에 어떻게 적용하면 좋을지도 고민해보면 좋을 것 같다."
  },
  {
    "objectID": "posts/1121_data_plan/index.html#개발환경설정",
    "href": "posts/1121_data_plan/index.html#개발환경설정",
    "title": "1121_Data Plan",
    "section": "개발환경설정",
    "text": "개발환경설정\n\n개발언어 : Python\nIDE : Anaconda, VSCode\n패키지 : numpy, pandas, scikit-learn, pytorch, transformers(hugging face)\n코드 공유 및 코드 저장소: github - cgbside repo"
  },
  {
    "objectID": "posts/1121_data_plan/index.html#eda",
    "href": "posts/1121_data_plan/index.html#eda",
    "title": "1121_Data Plan",
    "section": "EDA",
    "text": "EDA\n\n0. Source\n\nedaing.ipynb\nfinal.pdf\ncodebook_2022.xlsx\n\n\n\n1. Law_MST.csv 파일 로드\n- 349810행 호출 오류\n    \n\n💡 “251889”,“001740”,해양수산부,선원법,“0007001”,④,제7조,출항 전의 검사ㆍ보고의무 등,“제7조(출  “” (따옴표)로 감싸지 않아서 파싱 과정에서 오류가 발생 (잘못 복사된 파일) 파일의 EOF가 잘못 설정되어 이를 수정함 ⇒ 혹시 349810행 뒤에 짤린 부분이 있는지 확인 필요\n\n\n따라서, 이를 제외하고 csv 파일을 로드하였음.\n\ndata = pd.read_csv('law_mst.csv')\n- Output\n\n\n\n결과\n\n\n\n\n2. Feature 설명\n\n\n\n\n\n\n\n\nFeature\n설명\n처리방법\n\n\n\n\nlaw_seq\n법률, 시행규칙, 시행규칙 전체 연번\n정확히 무엇인지?\n\n\nlaw_id\n(법마다 다르다)\n정확히 무엇인지?\n\n\njdt_dept_nm\n소관부처 부처 개수, 부처명 등 파악하기\n\n\n\nlaw_kor_nm\n법령명\n\n\n\nprovision_key_no\n포맷 : _ _ _ _ _ _ _ (1) _ _ _ _ /_ _ 앞 6자리 숫자: 조(4자리)의 몇(2자리)ex) 제 1000조의 1 : 1000 / 01ex) 제 1조 : 0001 / 00ex) 제 15조의 19 : 0015 / 19(2) _ 맨 뒤 1자리 숫자: 0 -&gt; 장, 1-&gt; 장 아님\n-맨 뒤 숫자가 1인 경우, 조 번호만 추출-맨 뒤 숫자가 0인 경우, 행 삭제\n\n\nclause_no\n원숫자 O → 각 항번호 1 → 항 없이 호만 있음nan → 항과 호 모두 없는 법령\n\n\n\nprovision_no_nm\n조 한국어 표기(ex. 제1조, 제2조)\n\n\n\nprovision_title\n조 제목\n\n\n\nprovision_cont\n조 내용\n\n\n\nclause_cont\n항 내용\n\n\n\n\n\n\n\n참고 : 대한민국 법조문 체계\n\n\n+ 대한민국 정부 조직도 체계 참고사항으로 넣기\n+ 정부조직이 업데이트 시(승격 등), 어떻게 반영할 것인지?\n+ 시군구 업데이트 시(승격 등), 어떻게 반영할 것인지?\n+ 업데이트 될 때마다 바로 반영이 가능한가?\n+ 그리고 현재, Law_MST.csv에는 따로 사무여부 등의 레이블링이 되어있지 않은데 그렇다면 학습용데이터는 2019년 자료를 활용해야 하는 것인가?\n그렇다면.. 우리가 학습해야 할 데이터는.. 2019사무목록 최종본(인쇄용 최종).xlsx 파일인데 (레이블링이 되어 있으니) 그렇다면 Law_MST.csv는 어디에 사용하는 걸까?\n\n\n3. EDA - excel 파일\n\n결측치 확인: ‘항’, ‘비고’ column만 null값 존재\n\n\n\n\n출력 결과\n\n\n\nunique값 확인\n\n\n조문 제목과 사무 유형은 동일한 내용임에도 띄어쓰기가 다르게 되어 있는 경우 있었음 ⇒ replace 함수 통해 띄어쓰기 제거\n\n\n\n\ncolumn명\n원래 unique 값\n띄어쓰기 없앤 후 unique 값\n\n\n\n\n소관부서\n41\n41\n\n\n법령\n2859\n2859\n\n\n조문제목\n30034\n29544\n\n\n사무명\n56789\n56418\n\n\n사무유형\n33\n16\n\n\n비고\n4036\n3843\n\n\n\n+) 비고의 경우, 오탈자 존재(ex. 가획재정부 장관) / 동일한 기관 다른 표기 존재(ex.기획재정부 장관, 기재부 장관)\n\n\n4. EDA를 통해 세운 전처리/모델링 전략\n\n‘소관부처’, ‘법령’, ‘비고’ 라벨 인코딩하여 사무 유형 classification 진행해보고자 함\nlaw_mst.csv 파일과 2019사무목록 최종본.xlsx 파일의 연도 정보가 일치하지 않음\nlaw_mst.csv 파일과 2019사무목록 최종본.xlsx 파일을 ’조와 항’을 key로 하여 left join하면 사무인지 아닌지도 파악할 수 있음 (join한 후, 사무명, 사무유형, 비고가 NaN이면 비사무)\n\n⇒ 그러나, 이를 위해서는 2번 문제가 해결되어야 함\n\n사무명의 경우, 법령에서 괄호 속 텍스트 추출하는 방식으로 처리\n\n\n2019사무목록 최종본(인쇄용 최종).xlsx\n\n\n이 파일에는 아래와 같이 레이블링된 사무목록들이 들어있다. 하지만, 사무가 아닌 법령은 제거되어 있는 등(모든 법령들이 들어있지 않다!), 자체적으로 전처리가 되어있는 자료이다. 또한 법의 특성상(?) ’호’는 ’사무’에 대한 내용을 담고 있진 않은듯? 상위의 ’조’나 ’항’에서 ’사무명’을 추출할 수 있는 듯하다.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n연번\n소관부처\n법령\n조\n항\n조문제목\n조문\n사무명\n사무유형\n비고"
  },
  {
    "objectID": "posts/1121_data_plan/index.html#예측개발-방법론",
    "href": "posts/1121_data_plan/index.html#예측개발-방법론",
    "title": "1121_Data Plan",
    "section": "예측개발 방법론",
    "text": "예측개발 방법론\n&lt;개발 목적 및 프로세스&gt;\n\n사무 여부 O/X\n사무 여부가 O 일 때, 사무 수행 주체 파악\n→ 데이터 사전 구성? (부처에 뭐가 있는 지 미리 작성)\n위탁 여부 파악 O/X (위탁이 O인 경우 사무 수행 주체 소분류 컬럼 추가 생성 필요)\n사무 유형 파악\n\n—&gt; 부처명, 시군구 범위 변경 시에도 반영이 가능해야 함.\n&lt;목표: 사무유형 classification : 16개 &gt;\n**** 어떤 프로세스를 적용할까요?!***\n** 프로세스 : 문서 구조 분석 -&gt; 엔터티 추출 및 분류 모델(NER 모델을 통한 사무주체 추출) -&gt; ‘위임’,’위탁’등의 관계를 분석하는 알고리즘(종속관계 파악하는 관계 추출 기술…?) -&gt; 모델 학습(어떤 방법론이 가능할지는?) -&gt; 모델 성능 평가 및 하이퍼파라미터 조정 -&gt; 실제 환경에 통합 및 사용자 인터페이스를 통한 결과 제공*\n[시도 방법1] 문서 구조 분석 -&gt; 엔터티 추출 및 분류 모델(NER 모델을 통한 사무주체 추출) -&gt; ‘위임’,’위탁’등의 관계를 분석하는 알고리즘(종속관계 파악하는 관계 추출 기술…?) -&gt; 모델 학습(어떤 방법론이 가능할지는?) -&gt; 모델 성능 평가 및 하이퍼파라미터 조정 -&gt; 실제 환경에 통합 및 사용자 인터페이스를 통한 결과 제공\n[시도 방법2] ‘소관부처’, ‘법령’, ‘비고’ 라벨인코딩 ⇒ 머신러닝 기법(XGBoost, LightGBM, 여러 모델 앙상블) 적용\n[시도 방법3] huggingface에서 nlp 모델 사용 ⇒ 딥러닝 기법 시도"
  },
  {
    "objectID": "posts/1121_data_plan/index.html#모형설계서",
    "href": "posts/1121_data_plan/index.html#모형설계서",
    "title": "1121_Data Plan",
    "section": "모형설계서",
    "text": "모형설계서\n\n\n\n\n\n\n\n\n\n개발 목적/프로세스\n개발 방법론\n사용 모델\nperformance metric\n\n\n\n\n사무 여부 O/X\nrule-based classification\nif-elif 위임되면 사무X : ~령으로 정한다\nf1 score, recall(사무가 맞는데 사무가 아니라고 분류되는게 더 위험하기 때문에)\n\n\n사무 유형 파악\n\n\n\n\n\n사무 여부가 O 일 때, 사무 수행 주체 파악\ntrial1) 문장에서 주어 추출\n\n\n\n\ntrial2) 수행 주체 데이터 사전 구성, 데이터 사전에 있는 명사 모두 추출\n\naccuracy (수행주체 올바르게 추출한 데이터 수 / 전체 데이터 수)\n\n\n\n3) 위탁 여부 파악 O/X\nrule-based classification(위탁이 O이면 경우에 따라 사무가 X가 될 수 있음.)\n정규표현식 사용(‘~조에 따른’ 표현이 있으면 위탁여부 O로 classify)\nf1 score\n\n\n\n&lt;의문점!&gt;\n\n법령정보api는 새로운 조문이 업데이트 되면 이를 학습할 목적으로 사용하는 것인지?\n그럼 일단 학습은 기존의 2019, 2022년 정리된 조문을 바탕으로 학습하는 것인지?\n위임, 조, 규칙, 시행령 등 서로 종속되어 있는 관계는 어떻게 학습을 하는 것인지?\n\n\ntrain / validation / test set 구분\n\n\n2019사무목록 최종본(인쇄용 최종).xlsx 로 지도학습 실시 (확실하지 않음)\nQ: 그럼 Law_MST.csv 어디에 사용?\nLaw_MST.csv로 Demo 시연"
  },
  {
    "objectID": "posts/1120_hwajeong/index.html",
    "href": "posts/1120_hwajeong/index.html",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "최근에 시작한 프로젝트에서 처음으로 ’raw data’와 마주한 이야기를 해볼까 한다.\n\n\n간략하게 이 프로젝트에 대해서 소개를 하자면… 이 프로젝트의 목표는 AI를 활용하여 법령 사무조사 자동화의 가능성을 탐구하는 것이다. 구체적으로는 중앙행정기관과 지방자치단체 간의 ‘사무주체’, ‘사무유형’, ’사무명’을 추출하고, 이를 자동화하는 모형을 개발하는 것인데, 최종적으로는 새로운 법령이 업데이트되어도 사무를 자동으로 구분할 수 있는 시스템을 개발하는 것이 목표가 되겠다.\n\n\n\n이 프로젝트에서 처음으로 접한 것은 학습용으로 레이블링된 데이터와 Open API로 스크랩한 Raw 데이터였다. 데이터를 살펴보다 보니.. 이번 프로젝트는 단순히 조문에 대한 레이블을 학습하는 것을 넘어서, 모든 법령에 대한 구조적인 분석과 종속관계 파악이 필요하지 않을까? 라는 생각을 하게 되었다. 왜냐하면 법조문이라는 데이터의 특성상, 서로 종속된 법령들과(시행령, 시행규칙 등) 다양한 사무주체들이 복잡하게 얽혀 있다. 이 16가지 사무유형을 분류하기까지 너무 많은 경우의 수가 있어 쉽진 않을 것 같다는 생각이 들었다.  + 단순히 조문에 대한 레이블만 학습하면 될 문제인지? 아니면 모든 법령에 대해서 구조적인 분석을 통해 근본적으로 종속관계를 파악하고, 이를 반영할 수 있는 모델을 만들 것인지에 대한 고민이 생겼다.\n그래서 추가적으로, 유사한 프로젝트를 찾아보고 어떤 분석기법이 적합할지 탐색할 필요가 있다.\n\n\n\napi로 불러온 raw data의 전처리에 대한 이야기를 짧게 해 보자면, 우선적으로는 공백을 제거하는 등의 기본적인 전처리를 먼저 진행해야 하겠고, 뿐만 아니라 기존에 레이블링된 데이터는 주로 중앙행정기관과 지방자치단체의 법령만 다뤘지만, Raw 데이터에는 국회, 감사원 등 다양한 기관의 법령도 포함되어 있었기에 이에 대한 처리도 필요할 것으로 보인다.\n\n\n\n당장 1차적으로는 사무유형을 분류하는 데 주력해야겠지만 더 나아가서 앞으로 더 많은 활용을 위해서는 앞으로 법령정보 API를 활용하는 방법에 대해서도 더 고민해봐야겠다. 특히, 국가법령정보 공동활용 홈페이지에서 제공하는 Open API를 활용해, 업데이트 된 시행령을 반영하기 위해서는 [efYd] 변수를 이용해 자동화가 가능하도록 구성하는 방안 또한 필요해 보인다.\n\n\n\n이 프로젝트가 성공적으로 마무리되면, 그동안의 흥미 위주의 데이터 분석을 넘어서 실제 현안 해결에 기여할 수 있게 될 것 같아(작은 부분이라도..) 의미 있는 프로젝트가 될 것 같다. 현실의 복잡한 문제를 데이터 분석을 통해 해결해 나가는 과정을 통해 더 많이 배우고 성장할 수 있기를.…!"
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#프로젝트-소개",
    "href": "posts/1120_hwajeong/index.html#프로젝트-소개",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "간략하게 이 프로젝트에 대해서 소개를 하자면… 이 프로젝트의 목표는 AI를 활용하여 법령 사무조사 자동화의 가능성을 탐구하는 것이다. 구체적으로는 중앙행정기관과 지방자치단체 간의 ‘사무주체’, ‘사무유형’, ’사무명’을 추출하고, 이를 자동화하는 모형을 개발하는 것인데, 최종적으로는 새로운 법령이 업데이트되어도 사무를 자동으로 구분할 수 있는 시스템을 개발하는 것이 목표가 되겠다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#데이터-탐색-및-모형-개발-process에-대한-생각",
    "href": "posts/1120_hwajeong/index.html#데이터-탐색-및-모형-개발-process에-대한-생각",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "이 프로젝트에서 처음으로 접한 것은 학습용으로 레이블링된 데이터와 Open API로 스크랩한 Raw 데이터였다. 데이터를 살펴보다 보니.. 이번 프로젝트는 단순히 조문에 대한 레이블을 학습하는 것을 넘어서, 모든 법령에 대한 구조적인 분석과 종속관계 파악이 필요하지 않을까? 라는 생각을 하게 되었다. 왜냐하면 법조문이라는 데이터의 특성상, 서로 종속된 법령들과(시행령, 시행규칙 등) 다양한 사무유형이 복잡하게 얽혀 있기에, 이 16가지 사무유형을 분류하기까지 너무 많은 경우의 수가 있어 쉽진 않을 것 같다는 생각이 들었다. + 단순히 조문에 대한 레이블만 학습하면 될 문제인지? 아니면 모든 법령에 대해서 구조적인 분석을 통해 근본적으로 종속관계를 파악하고, 이를 반영할 수 있는 모델을 만들 것인지에 대한 고민이 생겼다.\n그래서 추가적으로, 유사한 프로젝트를 찾아보고 어떤 분석기법이 적합할지 탐색해 봐야겠다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#전처리",
    "href": "posts/1120_hwajeong/index.html#전처리",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "api로 불러온 raw data의 전처리에 대한 이야기를 짧게 해 보자면, 우선적으로는 공백을 제거하는 등의 기본적인 전처리를 먼저 진행해야 하겠고, 뿐만 아니라 기존에 레이블링된 데이터는 주로 중앙행정기관과 지방자치단체의 법령만 다뤘지만, Raw 데이터에는 국회, 감사원 등 다양한 기관의 법령도 포함되어 있었기에 이에 대한 처리도 필요할 것으로 보인다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#향후-고민할-점",
    "href": "posts/1120_hwajeong/index.html#향후-고민할-점",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "당장은 1차적으로는 사무유형을 분류하는 데 주력해야겠지만 더 나아가서 앞으로 더 많은 활용을 위해서는 앞으로 법령정보 API를 활용하는 방법에 대해서도 더 고민해봐야겠다. 특히, 국가법령정보 공동활용 홈페이지에서 제공하는 Open API를 활용해, 시행령 업데이트 시 [efYd] 변수를 이용해 자동 업데이트가 가능하도록 구성하는 방안 또한 필요해 보인다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#section",
    "href": "posts/1120_hwajeong/index.html#section",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "이 프로젝트가 성공적으로 마무리되면, 그동안의 흥미 위주의 데이터 분석을 넘어서 실제 현안 해결에 기여할 수 있게(작은 부분이라도..) 될 것 같아 현실의 복잡한 문제를 데이터 분석을 통해 해결해 나가는 과정을 통해 더 많이 배우고 성장할 수 있기를 …!"
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#데이터-및-모형-개발-process에-대한-생각",
    "href": "posts/1120_hwajeong/index.html#데이터-및-모형-개발-process에-대한-생각",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "이 프로젝트에서 처음으로 접한 것은 학습용으로 레이블링된 데이터와 Open API로 스크랩한 Raw 데이터였다. 데이터를 살펴보다 보니.. 이번 프로젝트는 단순히 조문에 대한 레이블을 학습하는 것을 넘어서, 모든 법령에 대한 구조적인 분석과 종속관계 파악이 필요하지 않을까? 라는 생각을 하게 되었다. 왜냐하면 법조문이라는 데이터의 특성상, 서로 종속된 법령들과(시행령, 시행규칙 등) 다양한 사무주체들이 복잡하게 얽혀 있다. 이 16가지 사무유형을 분류하기까지 너무 많은 경우의 수가 있어 쉽진 않을 것 같다는 생각이 들었다.  + 단순히 조문에 대한 레이블만 학습하면 될 문제인지? 아니면 모든 법령에 대해서 구조적인 분석을 통해 근본적으로 종속관계를 파악하고, 이를 반영할 수 있는 모델을 만들 것인지에 대한 고민이 생겼다.\n그래서 추가적으로, 유사한 프로젝트를 찾아보고 어떤 분석기법이 적합할지 탐색할 필요가 있다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#향후-고민해보아야-할-점들",
    "href": "posts/1120_hwajeong/index.html#향후-고민해보아야-할-점들",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "당장 1차적으로는 사무유형을 분류하는 데 주력해야겠지만 더 나아가서 앞으로 더 많은 활용을 위해서는 앞으로 법령정보 API를 활용하는 방법에 대해서도 더 고민해봐야겠다. 특히, 국가법령정보 공동활용 홈페이지에서 제공하는 Open API를 활용해, 업데이트 된 시행령을 반영하기 위해서는 [efYd] 변수를 이용해 자동화가 가능하도록 구성하는 방안 또한 필요해 보인다."
  },
  {
    "objectID": "posts/1120_hwajeong/index.html#마무리하며..",
    "href": "posts/1120_hwajeong/index.html#마무리하며..",
    "title": "1121_Hwajeong",
    "section": "",
    "text": "이 프로젝트가 성공적으로 마무리되면, 그동안의 흥미 위주의 데이터 분석을 넘어서 실제 현안 해결에 기여할 수 있게 될 것 같아(작은 부분이라도..) 의미 있는 프로젝트가 될 것 같다. 현실의 복잡한 문제를 데이터 분석을 통해 해결해 나가는 과정을 통해 더 많이 배우고 성장할 수 있기를.…!"
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "데이터",
    "section": "",
    "text": "Base Table 만들기\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html",
    "href": "data_posts/MakeBaseTable.html",
    "title": "Base Table 만들기",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport numpy as np\nimport pandas as pd\n\n\n# 데이터 load\ndata = pd.read_csv('main_data.csv')\n\n/var/folders/sy/5dw5r1ys5fdb3h0gbq8x0g6m0000gn/T/ipykernel_47971/1248266004.py:2: DtypeWarning: Columns (2,4,5,8,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('main_data.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861719 entries, 0 to 861718\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861666 non-null  object \n 1   법령명        861702 non-null  object \n 2   법령구분       861704 non-null  object \n 3   조번호        861129 non-null  object \n 4   항번호        666590 non-null  object \n 5   호번호        504904 non-null  object \n 6   조문제목       805869 non-null  object \n 7   조문         848856 non-null  object \n 8   사무판단       767124 non-null  object \n 9   사무판단근거     700871 non-null  object \n 10  사무명        60113 non-null   object \n 11  수행주체       60116 non-null   object \n 12  사무유형       60071 non-null   object \n 13  위임사무판단     761139 non-null  object \n 14  위임근거규정     5311 non-null    object \n 15  수임기관       4416 non-null    object \n 16  특행기관       93089 non-null   object \n 17  재위임사무판단    702502 non-null  object \n 18  재위임근거규정    54 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758703 non-null  float64\n 21  위탁근거규정     3975 non-null    object \n 22  수탁기관       3952 non-null    object \n 23  사무유형(소분류)  60114 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(1), object(24)\nmemory usage: 164.4+ MB"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#라이브러리-설치-및-데이터-불러오기",
    "href": "data_posts/MakeBaseTable.html#라이브러리-설치-및-데이터-불러오기",
    "title": "Base Table 만들기",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport numpy as np\nimport pandas as pd\n\n\n# 데이터 load\ndata = pd.read_csv('main_data.csv')\n\n/var/folders/sy/5dw5r1ys5fdb3h0gbq8x0g6m0000gn/T/ipykernel_47971/1248266004.py:2: DtypeWarning: Columns (2,4,5,8,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('main_data.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861719 entries, 0 to 861718\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861666 non-null  object \n 1   법령명        861702 non-null  object \n 2   법령구분       861704 non-null  object \n 3   조번호        861129 non-null  object \n 4   항번호        666590 non-null  object \n 5   호번호        504904 non-null  object \n 6   조문제목       805869 non-null  object \n 7   조문         848856 non-null  object \n 8   사무판단       767124 non-null  object \n 9   사무판단근거     700871 non-null  object \n 10  사무명        60113 non-null   object \n 11  수행주체       60116 non-null   object \n 12  사무유형       60071 non-null   object \n 13  위임사무판단     761139 non-null  object \n 14  위임근거규정     5311 non-null    object \n 15  수임기관       4416 non-null    object \n 16  특행기관       93089 non-null   object \n 17  재위임사무판단    702502 non-null  object \n 18  재위임근거규정    54 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758703 non-null  float64\n 21  위탁근거규정     3975 non-null    object \n 22  수탁기관       3952 non-null    object \n 23  사무유형(소분류)  60114 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(1), object(24)\nmemory usage: 164.4+ MB"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#설명변수소관부처명-법령명-조번호-항번호-호번호-조문제목-조문가-모두-결측치인-행-삭제",
    "href": "data_posts/MakeBaseTable.html#설명변수소관부처명-법령명-조번호-항번호-호번호-조문제목-조문가-모두-결측치인-행-삭제",
    "title": "Base Table 만들기",
    "section": "1) 설명변수(소관부처명, 법령명, 조번호, 항번호, 호번호, 조문제목, 조문)가 모두 결측치인 행 삭제",
    "text": "1) 설명변수(소관부처명, 법령명, 조번호, 항번호, 호번호, 조문제목, 조문)가 모두 결측치인 행 삭제"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#section",
    "href": "data_posts/MakeBaseTable.html#section",
    "title": "Base Table 만들기",
    "section": "",
    "text": "def x_null_drop(df): \n    select_column = ['소관부처명', '법령명', '조번호', '항번호', '호번호', '조문제목', '조문']\n    delete_row_idx = list(df[df[select_column].isnull().all(axis = 1)].index)\n    delete_row_idx.sort(reverse = True)\n    for i in delete_row_idx:\n        df = df.drop([i],axis = 0)\n    return df\n\n\ndata = x_null_drop(data)\n\n(861711, 25)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#소관부처명-결측치-처리",
    "href": "data_posts/MakeBaseTable.html#소관부처명-결측치-처리",
    "title": "Base Table 만들기",
    "section": "2) 소관부처명 결측치 처리",
    "text": "2) 소관부처명 결측치 처리\n\n소관부처명 결측치: 45개\n동일한 법령에 대해서는 동일한 소관부처를 가짐\n이에, 다른 행 중 동일한 법령을 지닌 소관부처 파악 후 결측치 채워줌\n\n\ndef dep_law_preprocessing(df):\n        # department_idx: '법령명'은 채워져있는데 '소관부처명'은 채워져있지 않은 행의 index\n        department_idx = df[df['소관부처명'].isnull() & df['법령명'].notnull()].index\n        # department_name_list: '소관부처명'이 채워져야할 법령명\n        department_name_list = df[df['소관부처명'].isnull() & df['법령명'].notnull()]['법령명'].unique()\n\n        department_dic = {}\n        department_dic['건설산업기본법'] = '국토교통부'\n        department_dic['보건범죄단속에관한특별조치법시행령'] = '보건복지부'\n        department_dic['항로표지법'] = '해양수산부'\n        department_dic['수산자원관리법'] = '해양수산부'\n        department_dic['연안관리법'] = '해양수산부'\n        department_dic['야생생물 보호 및 관리에 관한 법률'] = '환경부'\n\n        for i in range(len(department_idx)):\n                for j in range(len(department_name_list)):\n                        df.loc[department_idx[i],'소관부처명'] = department_dic[department_name_list[j]]\n\n        # '소관부처명', '법령명' 모두 채워져있지 않은 행\n        \n        return df\n\n\ndata = dep_law_preprocessing(data)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#법령명-결측치-처리",
    "href": "data_posts/MakeBaseTable.html#법령명-결측치-처리",
    "title": "Base Table 만들기",
    "section": "3) 법령명 결측치 처리",
    "text": "3) 법령명 결측치 처리\n\n조, 항, 조문 통해 법령명 찾아 삽입\n\n\ndef law_name_preprocessing(df):\n    idx = 14168\n    df.loc[idx,'소관부처명'] = '고용노동부'\n    df.loc[idx,'법령명'] = '근로자퇴직급여 보장법'\n    df.loc[idx,'법령구분'] = 1\n\n    idx = 198519\n    df.loc[idx,'소관부처명'] = '국토교통부'\n    df.loc[idx,'법령명'] = '택수운송사업의 발전에 관한 법률'\n    df.loc[idx,'법령구분'] = 1\n    df.loc[idx,'조번호'] = 11\n    df.loc[idx,'항번호'] = 1\n    df.loc[idx,'조문제목'] = '감차계획의 수립 및 시행 등'\n    idx = 686791\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '수산업ㆍ어촌 공익기능 증진을 위한 직접지불제도 운영에 관한 법률'\n\n    idx = 708300\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '해양공간계획 및 관리에 관한 법률'\n\n    idx = 708831\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '해양폐기물 및 해양오염퇴적물 관리법'\n\n    idx = 766079\n    df.loc[idx,'소관부처명'] = '행정안전부'\n    df.loc[idx,'법령명'] = '새마을금고법'\n\n    idx = 859679\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n\n    idx = 859692\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n\n    idx = 859755\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n    \n    return df\n\n\ndata = law_name_preprocessing(data)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#법령구분-처리",
    "href": "data_posts/MakeBaseTable.html#법령구분-처리",
    "title": "Base Table 만들기",
    "section": "4) 법령구분 처리",
    "text": "4) 법령구분 처리\n\n법령구분 결측치 처리\n법령구분 자료형 int로 통일\n\n\ndef law_category_preprocessing(df):\n    # '법령구분'이 결측치인 행들의 index\n    null_idx = df[df['법령구분'].isnull()].index\n    # 국가법령정보센터 확인 결과, 결측치인 모든 행들은 법률, 즉 '1'에 해당\n    for i in null_idx:\n        df.loc[i,'법령구분'] = 1\n    \n    # 국가법령정보센터 확인 결과, '법령구분'이 공백으로 되어있는 행은 시행령, 즉 '2'에 해당\n    df.loc[df['법령구분']==' ', \"법령구분\"] = 2\n\n    # 법령구분 자료형 통일\n    df['법령구분'] = df['법령구분'].astype('int64')\n\n    return df\n\n\ndata = law_category_preprocessing(data)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#사무판단-처리",
    "href": "data_posts/MakeBaseTable.html#사무판단-처리",
    "title": "Base Table 만들기",
    "section": "5) 사무판단 처리",
    "text": "5) 사무판단 처리\n\n’ ’ -&gt; nan, ‘0’ -&gt; 0 , ‘1’ -&gt; 1, ‘0 1’ -&gt; 2 float 형태로 변환\n\n\ndef decision_preprocessing(df):\n    # 표기방식 통일\n    idx_nan = df[(df['사무판단'] == ' ')].index #idx_nan: '사무판단'이 nan인 행의 index\n    for i in idx_nan:\n        df.loc[i,'사무판단'] = np.nan\n    idx_0 = df[(df['사무판단'] == '0')].index #idx_0: '사무판단'이 '0'인 행의 index\n    for i in idx_0:\n        df.loc[i,'사무판단'] = 0\n    idx_1 = df[(df['사무판단'] == '1')].index #idx_1: '사무판단'이 '1'인 행의 index\n    for i in idx_1:\n        df.loc[i,'사무판단'] = 1\n    idx_2 = df[(df['사무판단'] == '0 1')].index #idx_2: '사무판단'이 '0 1'인 행의 index\n    for i in idx_2:\n        df.loc[i,'사무판단'] = 2\n\n    # 오류 행 삭제\n    ## 경우1: 사무가 아님에도 사무 유형이 분류된 경우\n    delete_0_idx = list((df[(df['사무판단'] == 0)  & (df['사무유형(소분류)'].notna())]).index)\n    df = df.drop(delete_0_idx, axis = 0)\n    \n    ## 경우2: 사무임에도 사무 유형이 분류되지 않은 경우\n    delete_1_idx1 = list((df[(df['사무판단'] == 1)  & (df['사무유형'].isnull())]).index)\n    df = df.drop(delete_1_idx1, axis = 0)\n    \n    delete_1_idx2 = list((df[(df['사무판단'] == 1)  & (df['사무유형(소분류)'].isnull())]).index)\n    df = df.drop(delete_1_idx2, axis = 0)\n\n    # 결측행 처리\n    ## 경우1: 사무 유형이 분류된 경우 =&gt; '1'로 채움\n    change_1_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].notna()) & (df['사무유형(소분류)'].notna())].index\n    df.loc[change_1_idx, '사무판단'] = 1\n    \n    # 경우2: 사무 유형이 분류되지 않은 경우 =&gt; '0'으로 채움\n    change_0_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].isnull()) & (df['사무유형(소분류)'].isnull())].index\n    df.loc[change_0_idx,'사무판단'] = 0\n    \n    # 자료형 통일\n    df['사무판단'] = df['사무판단'].astype('int64')\n\n    return df\n\n\ndata = decision_preprocessing(data)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#소관부처명-공백-처리",
    "href": "data_posts/MakeBaseTable.html#소관부처명-공백-처리",
    "title": "Base Table 만들기",
    "section": "6) 소관부처명 공백 처리",
    "text": "6) 소관부처명 공백 처리\n\ndef blank_preprocessing(df):\n    df.loc[df['소관부처명']==\"교육부,\\n고용노동부\", \"소관부처명\"] = '고용노동부,교육부'\n    df.loc[df['소관부처명']==\"과학기술정보통신부, \\n교육부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    df.loc[df['소관부처명']==\"교육부,\\n과학기술정보통신부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    return df\n    \n\n\ndata = blank_preprocessing(data)"
  },
  {
    "objectID": "data_posts/MakeBaseTable.html#조문-조문-제목-결측치-처리",
    "href": "data_posts/MakeBaseTable.html#조문-조문-제목-결측치-처리",
    "title": "Base Table 만들기",
    "section": "7) 조문, 조문 제목 결측치 처리",
    "text": "7) 조문, 조문 제목 결측치 처리\n\n조문, 조문 제목 null값이면 ’0’으로 채움\n\n\ndef law_preprocessing(df):\n    df.loc[df['조문제목'].isna(), '조문제목'] = '0'\n    df.loc[df['조문'].isna(), '조문'] = '0'\n    return df\n\n\ndata = law_preprocessing(data)"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "개발개요",
    "section": "",
    "text": "Title\n\n\nAuthor\n\n\n\n\n\n\n\nBase Table 만들기\n\n\n\n\n\n\n\n\nEDA ( BaseTable_2.csv 사용 )\n\n\n\n\n\n\n\n딥러닝 코드 정리\n\n\n\n\n\n\n\n앙상블 코드 정리\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html",
    "href": "dev_posts/MakeBaseTable.html",
    "title": "Base Table 만들기",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport numpy as np\nimport pandas as pd\n\n\n# 데이터 load\ndata = pd.read_csv('main_data.csv')\n\n/var/folders/zr/_f1rgf8n0w3541q9k8p3smhr0000gn/T/ipykernel_87762/1248266004.py:2: DtypeWarning: Columns (2,4,5,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('main_data.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861719 entries, 0 to 861718\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861666 non-null  object \n 1   법령명        861702 non-null  object \n 2   법령구분       861704 non-null  object \n 3   조번호        861129 non-null  object \n 4   항번호        666590 non-null  object \n 5   호번호        504904 non-null  object \n 6   조문제목       805869 non-null  object \n 7   조문         848856 non-null  object \n 8   사무판단       767124 non-null  float64\n 9   사무판단근거     700871 non-null  object \n 10  사무명        60113 non-null   object \n 11  수행주체       60116 non-null   object \n 12  사무유형       60071 non-null   object \n 13  위임사무판단     761139 non-null  object \n 14  위임근거규정     5311 non-null    object \n 15  수임기관       4416 non-null    object \n 16  특행기관       93089 non-null   object \n 17  재위임사무판단    702502 non-null  object \n 18  재위임근거규정    54 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758703 non-null  float64\n 21  위탁근거규정     3975 non-null    object \n 22  수탁기관       3952 non-null    object \n 23  사무유형(소분류)  60114 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(2), object(23)\nmemory usage: 164.4+ MB"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#라이브러리-설치-및-데이터-불러오기",
    "href": "dev_posts/MakeBaseTable.html#라이브러리-설치-및-데이터-불러오기",
    "title": "Base Table 만들기",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport numpy as np\nimport pandas as pd\n\n\n# 데이터 load\ndata = pd.read_csv('main_data.csv')\n\n/var/folders/zr/_f1rgf8n0w3541q9k8p3smhr0000gn/T/ipykernel_87762/1248266004.py:2: DtypeWarning: Columns (2,4,5,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('main_data.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861719 entries, 0 to 861718\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861666 non-null  object \n 1   법령명        861702 non-null  object \n 2   법령구분       861704 non-null  object \n 3   조번호        861129 non-null  object \n 4   항번호        666590 non-null  object \n 5   호번호        504904 non-null  object \n 6   조문제목       805869 non-null  object \n 7   조문         848856 non-null  object \n 8   사무판단       767124 non-null  float64\n 9   사무판단근거     700871 non-null  object \n 10  사무명        60113 non-null   object \n 11  수행주체       60116 non-null   object \n 12  사무유형       60071 non-null   object \n 13  위임사무판단     761139 non-null  object \n 14  위임근거규정     5311 non-null    object \n 15  수임기관       4416 non-null    object \n 16  특행기관       93089 non-null   object \n 17  재위임사무판단    702502 non-null  object \n 18  재위임근거규정    54 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758703 non-null  float64\n 21  위탁근거규정     3975 non-null    object \n 22  수탁기관       3952 non-null    object \n 23  사무유형(소분류)  60114 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(2), object(23)\nmemory usage: 164.4+ MB"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#설명변수소관부처명-법령명-조번호-항번호-호번호-조문제목-조문가-모두-결측치인-행-삭제",
    "href": "dev_posts/MakeBaseTable.html#설명변수소관부처명-법령명-조번호-항번호-호번호-조문제목-조문가-모두-결측치인-행-삭제",
    "title": "Base Table 만들기",
    "section": "1) 설명변수(소관부처명, 법령명, 조번호, 항번호, 호번호, 조문제목, 조문)가 모두 결측치인 행 삭제",
    "text": "1) 설명변수(소관부처명, 법령명, 조번호, 항번호, 호번호, 조문제목, 조문)가 모두 결측치인 행 삭제\n\ndef x_null_drop(df): \n    select_column = ['소관부처명', '법령명', '조번호', '항번호', '호번호', '조문제목', '조문']\n    delete_row_idx = list(df[df[select_column].isnull().all(axis = 1)].index)\n    delete_row_idx.sort(reverse = True)\n    for i in delete_row_idx:\n        df = df.drop([i],axis = 0)\n    return df\n\n\ndata = x_null_drop(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#section",
    "href": "dev_posts/MakeBaseTable.html#section",
    "title": "Base Table 만들기",
    "section": "",
    "text": "def x_null_drop(df): \n    select_column = ['소관부처명', '법령명', '조번호', '항번호', '호번호', '조문제목', '조문']\n    delete_row_idx = list(df[df[select_column].isnull().all(axis = 1)].index)\n    delete_row_idx.sort(reverse = True)\n    for i in delete_row_idx:\n        df = df.drop([i],axis = 0)\n    return df\n\n\ndata = x_null_drop(data)\n\n(861711, 25)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#소관부처명-결측치-처리",
    "href": "dev_posts/MakeBaseTable.html#소관부처명-결측치-처리",
    "title": "Base Table 만들기",
    "section": "2) 소관부처명 결측치 처리",
    "text": "2) 소관부처명 결측치 처리\n\n소관부처명 결측치: 45개\n동일한 법령에 대해서는 동일한 소관부처를 가짐\n이에, 다른 행 중 동일한 법령을 지닌 소관부처 파악 후 결측치 채워줌\n소관부처명 중복 처리\n\n\ndef dep_law_preprocessing(df):\n        # department_idx: '법령명'은 채워져있는데 '소관부처명'은 채워져있지 않은 행의 index\n        department_idx = df[df['소관부처명'].isnull() & df['법령명'].notnull()].index\n        # department_name_list: '소관부처명'이 채워져야할 법령명\n        department_name_list = df[df['소관부처명'].isnull() & df['법령명'].notnull()]['법령명'].unique()\n\n        department_dic = {}\n        department_dic['건설산업기본법'] = '국토교통부'\n        department_dic['보건범죄단속에관한특별조치법시행령'] = '보건복지부'\n        department_dic['항로표지법'] = '해양수산부'\n        department_dic['수산자원관리법'] = '해양수산부'\n        department_dic['연안관리법'] = '해양수산부'\n        department_dic['야생생물 보호 및 관리에 관한 법률'] = '환경부'\n\n        for i in range(len(department_idx)):\n                for j in range(len(department_name_list)):\n                        df.loc[department_idx[i],'소관부처명'] = department_dic[department_name_list[j]]\n\n        # '소관부처명', '법령명' 모두 채워져있지 않은 행\n        \n        return df\n\n\ndef blank_preprocessing(df):\n    df.loc[df['소관부처명']==\"교육부,\\n고용노동부\", \"소관부처명\"] = '고용노동부,교육부'\n    df.loc[df['소관부처명']==\"과학기술정보통신부, \\n교육부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    df.loc[df['소관부처명']==\"교육부,\\n과학기술정보통신부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    return df\n\n\ndata = dep_law_preprocessing(data)\ndata = blank_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#법령명-결측치-처리",
    "href": "dev_posts/MakeBaseTable.html#법령명-결측치-처리",
    "title": "Base Table 만들기",
    "section": "3) 법령명 결측치 처리",
    "text": "3) 법령명 결측치 처리\n\n조, 항, 조문 통해 법령명 찾아 삽입\n\n\ndef law_name_preprocessing(df):\n    idx = 14168\n    df.loc[idx,'소관부처명'] = '고용노동부'\n    df.loc[idx,'법령명'] = '근로자퇴직급여 보장법'\n    df.loc[idx,'법령구분'] = 1\n\n    idx = 198519\n    df.loc[idx,'소관부처명'] = '국토교통부'\n    df.loc[idx,'법령명'] = '택수운송사업의 발전에 관한 법률'\n    df.loc[idx,'법령구분'] = 1\n    df.loc[idx,'조번호'] = 11\n    df.loc[idx,'항번호'] = 1\n    df.loc[idx,'조문제목'] = '감차계획의 수립 및 시행 등'\n    idx = 686791\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '수산업ㆍ어촌 공익기능 증진을 위한 직접지불제도 운영에 관한 법률'\n\n    idx = 708300\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '해양공간계획 및 관리에 관한 법률'\n\n    idx = 708831\n    df.loc[idx,'소관부처명'] = '해양수산부'\n    df.loc[idx,'법령명'] = '해양폐기물 및 해양오염퇴적물 관리법'\n\n    idx = 766079\n    df.loc[idx,'소관부처명'] = '행정안전부'\n    df.loc[idx,'법령명'] = '새마을금고법'\n\n    idx = 859679\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n\n    idx = 859692\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n\n    idx = 859755\n    df.loc[idx,'소관부처명'] = '환경부'\n    df.loc[idx,'법령명'] = '미세먼지 저감 및 관리에 관한 특별법'\n    \n    return df\n\n\ndata = law_name_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#법령구분-처리",
    "href": "dev_posts/MakeBaseTable.html#법령구분-처리",
    "title": "Base Table 만들기",
    "section": "4) 법령구분 처리",
    "text": "4) 법령구분 처리\n\n법령구분 결측치 처리\n법령구분 자료형 int로 통일\n\n\ndef law_category_preprocessing(df):\n    # '법령구분'이 결측치인 행들의 index\n    null_idx = df[df['법령구분'].isnull()].index\n    # 국가법령정보센터 확인 결과, 결측치인 모든 행들은 법률, 즉 '1'에 해당\n    for i in null_idx:\n        df.loc[i,'법령구분'] = 1\n    \n    # 국가법령정보센터 확인 결과, '법령구분'이 공백으로 되어있는 행은 시행령, 즉 '2'에 해당\n    df.loc[df['법령구분']==' ', \"법령구분\"] = 2\n\n    # 법령구분 자료형 통일\n    df['법령구분'] = df['법령구분'].astype('int64')\n\n    return df\n\n\ndef change_law(df):\n        change_laws = ['개발제한구역의 지정 및 관리에 관한 특별조치법', '건설기계관리법', '건설산업기본법', '건설산업기본법 시행령', \n                   '건축물의 분양에 관한 법률', '식품ㆍ의약품 등의 안전기술 진흥법 시행규칙', '위생용품 관리법', '지방세징수법 시행규칙',\n                   '대한민국과 아메리카합중국 간의 상호방위조약 제4조에 의한 시설과 구역 및 대한민국에서의 합중국 군대의 지위에 관한 협정의 시행에 관한 민사특별법 시행규칙',\n                   '대한민국과아메리카합중국간의상호방위조약제4조에의한시설과구역및대한민국에서의합중국군대의지위에관한협정의시행에관한민사특별법시행령']\n        remain_value = [2, 2, 2, 1, 0, 2, 2, 2, 0, 0]   # 수정해야 하는 값\n        change_value = [1, 1, 1, 2, 1, 3, 1, 3, 3, 2]   # 수정할 값\n        change_list = []\n        \n        for i in range(len(change_laws)):\n            if i==4:\n                idxs = df[(df['법령명'] == change_laws[i]) & (df['법령구분'] != 1)].index\n            elif i &gt; 7:\n                idxs = df[(df['법령명'] == change_laws[i])].index\n            else:\n                idxs = df[(df['법령명'] == change_laws[i]) & (df['법령구분'] == remain_value[i])].index\n            for idx in idxs:\n                change_list.append([idx, change_value[i]])\n        \n        # 법령 구분 값 변경\n        for change in change_list:\n            df.loc[change[0], \"법령구분\"] = change[1]\n        \n        return df\n\n\ndata = law_category_preprocessing(data)\ndata = change_law(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#사무판단-처리",
    "href": "dev_posts/MakeBaseTable.html#사무판단-처리",
    "title": "Base Table 만들기",
    "section": "5) 사무판단 처리",
    "text": "5) 사무판단 처리\n\n’ ’ -&gt; nan, ‘0’ -&gt; 0 , ‘1’ -&gt; 1, ‘0 1’ -&gt; 2 float 형태로 변환\n\n\ndef decision_preprocessing(df):\n    # 표기방식 통일\n    idx_nan = df[(df['사무판단'] == ' ')].index #idx_nan: '사무판단'이 nan인 행의 index\n    for i in idx_nan:\n        df.loc[i,'사무판단'] = np.nan\n    idx_0 = df[(df['사무판단'] == '0')].index #idx_0: '사무판단'이 '0'인 행의 index\n    for i in idx_0:\n        df.loc[i,'사무판단'] = 0\n    idx_1 = df[(df['사무판단'] == '1')].index #idx_1: '사무판단'이 '1'인 행의 index\n    for i in idx_1:\n        df.loc[i,'사무판단'] = 1\n    idx_2 = df[(df['사무판단'] == '0 1')].index #idx_2: '사무판단'이 '0 1'인 행의 index\n    for i in idx_2:\n        df.loc[i,'사무판단'] = 2\n\n    # 오류 행 삭제\n    ## 경우1: 사무가 아님에도 사무 유형이 분류된 경우\n    delete_0_idx = list((df[(df['사무판단'] == 0)  & (df['사무유형(소분류)'].notna())]).index)\n    df = df.drop(delete_0_idx, axis = 0)\n    \n    ## 경우2: 사무임에도 사무 유형이 분류되지 않은 경우\n    delete_1_idx1 = list((df[(df['사무판단'] == 1)  & (df['사무유형'].isnull())]).index)\n    df = df.drop(delete_1_idx1, axis = 0)\n    \n    delete_1_idx2 = list((df[(df['사무판단'] == 1)  & (df['사무유형(소분류)'].isnull())]).index)\n    df = df.drop(delete_1_idx2, axis = 0)\n\n    # 결측행 처리\n    ## 경우1: 사무 유형이 분류된 경우 =&gt; '1'로 채움\n    change_1_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].notna()) & (df['사무유형(소분류)'].notna())].index\n    df.loc[change_1_idx, '사무판단'] = 1\n    \n    # 경우2: 사무 유형이 분류되지 않은 경우 =&gt; '0'으로 채움\n    change_0_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].isnull()) & (df['사무유형(소분류)'].isnull())].index\n    df.loc[change_0_idx,'사무판단'] = 0\n    \n    # 자료형 통일\n    df['사무판단'] = df['사무판단'].astype('int64')\n\n    return df\n\n\ndata = decision_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#소관부처명-공백-처리",
    "href": "dev_posts/MakeBaseTable.html#소관부처명-공백-처리",
    "title": "Base Table 만들기",
    "section": "6) 소관부처명 공백 처리",
    "text": "6) 소관부처명 공백 처리\n\ndef blank_preprocessing(df):\n    df.loc[df['소관부처명']==\"교육부,\\n고용노동부\", \"소관부처명\"] = '고용노동부,교육부'\n    df.loc[df['소관부처명']==\"과학기술정보통신부, \\n교육부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    df.loc[df['소관부처명']==\"교육부,\\n과학기술정보통신부\", \"소관부처명\"] = '과학기술정보통신부,교육부'\n    return df\n    \n\n\ndata = blank_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#조문-조문-제목-결측치-처리",
    "href": "dev_posts/MakeBaseTable.html#조문-조문-제목-결측치-처리",
    "title": "Base Table 만들기",
    "section": "7) 조문, 조문 제목 결측치 처리",
    "text": "7) 조문, 조문 제목 결측치 처리\n\n조문, 조문 제목 null값이면 ’0’으로 채움\n\n\ndef law_preprocessing(df):\n    df.loc[df['조문제목'].isna(), '조문제목'] = '0'\n    df.loc[df['조문'].isna(), '조문'] = '0'\n    return df\n\n\ndata = law_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html",
    "href": "dev_posts/BaseTable2_EDA.html",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom matplotlib import rc\n%matplotlib inline\n\nrc('font', family='AppleGothic')\nplt.rcParams['axes.unicode_minus'] = False\n\n\n# 데이터 load\ndata = pd.read_csv('BaseTable_2.csv')\n\n/var/folders/fs/zfypqyv96hs22x794hfx0ycm0000gn/T/ipykernel_16581/684496263.py:2: DtypeWarning: Columns (4,5,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('BaseTable_2.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861624 entries, 0 to 861623\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861624 non-null  object \n 1   법령명        861624 non-null  object \n 2   법령구분       861624 non-null  int64  \n 3   조번호        861043 non-null  object \n 4   항번호        666511 non-null  object \n 5   호번호        504874 non-null  object \n 6   조문제목       861624 non-null  object \n 7   조문         861624 non-null  object \n 8   사무판단       861624 non-null  int64  \n 9   사무판단근거     700810 non-null  object \n 10  사무명        60068 non-null   object \n 11  수행주체       60069 non-null   object \n 12  사무유형       60026 non-null   object \n 13  위임사무판단     761044 non-null  object \n 14  위임근거규정     5292 non-null    object \n 15  수임기관       4398 non-null    object \n 16  특행기관       93064 non-null   object \n 17  재위임사무판단    702426 non-null  object \n 18  재위임근거규정    53 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758608 non-null  float64\n 21  위탁근거규정     3970 non-null    object \n 22  수탁기관       3943 non-null    object \n 23  사무유형(소분류)  60026 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(1), int64(2), object(22)\nmemory usage: 164.3+ MB"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#라이브러리-설치-및-데이터-불러오기",
    "href": "dev_posts/BaseTable2_EDA.html#라이브러리-설치-및-데이터-불러오기",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "",
    "text": "# 필요 라이브러리 설치\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as fm\nfrom matplotlib import rc\n%matplotlib inline\n\nrc('font', family='AppleGothic')\nplt.rcParams['axes.unicode_minus'] = False\n\n\n# 데이터 load\ndata = pd.read_csv('BaseTable_2.csv')\n\n/var/folders/fs/zfypqyv96hs22x794hfx0ycm0000gn/T/ipykernel_16581/684496263.py:2: DtypeWarning: Columns (4,5,13,16,17,18,19,24) have mixed types. Specify dtype option on import or set low_memory=False.\n  data = pd.read_csv('BaseTable_2.csv')\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 861624 entries, 0 to 861623\nData columns (total 25 columns):\n #   Column     Non-Null Count   Dtype  \n---  ------     --------------   -----  \n 0   소관부처명      861624 non-null  object \n 1   법령명        861624 non-null  object \n 2   법령구분       861624 non-null  int64  \n 3   조번호        861043 non-null  object \n 4   항번호        666511 non-null  object \n 5   호번호        504874 non-null  object \n 6   조문제목       861624 non-null  object \n 7   조문         861624 non-null  object \n 8   사무판단       861624 non-null  int64  \n 9   사무판단근거     700810 non-null  object \n 10  사무명        60068 non-null   object \n 11  수행주체       60069 non-null   object \n 12  사무유형       60026 non-null   object \n 13  위임사무판단     761044 non-null  object \n 14  위임근거규정     5292 non-null    object \n 15  수임기관       4398 non-null    object \n 16  특행기관       93064 non-null   object \n 17  재위임사무판단    702426 non-null  object \n 18  재위임근거규정    53 non-null      object \n 19  재수임기관      13 non-null      object \n 20  위탁사무판단     758608 non-null  float64\n 21  위탁근거규정     3970 non-null    object \n 22  수탁기관       3943 non-null    object \n 23  사무유형(소분류)  60026 non-null   object \n 24  기타         3 non-null       object \ndtypes: float64(1), int64(2), object(22)\nmemory usage: 164.3+ MB"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#사무판단-개수-파악",
    "href": "dev_posts/BaseTable2_EDA.html#사무판단-개수-파악",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "1) 사무판단 개수 파악",
    "text": "1) 사무판단 개수 파악\n\n사무판단 종류: 3개\n\n사무판단이 0인 경우 : 801598개\n사무판단이 1인 경우 : 60023개\n사무판단이 2인 경우 : 3개\n\n사무판단이 2인 경우 빼고 0과 1인 경우만 비교하기\n\n\ndef judgment_graph(df):\n    # 전체 갯수\n    N = len(df)\n    \n    # 0의 비율, 1의 비율 계산하기\n    ratio_0 = (len(df[df['사무판단'] == 0]))/N *100\n    print(f\" 0의 비율 : {ratio_0}\")\n    ratio_1 = (len(df[df['사무판단'] == 1]))/N *100\n    print(f\" 1의 비율 : {ratio_1}\")\n    \n    # pie chart 생성\n    ratio = [ratio_0, ratio_1]\n    labels = [0,1]\n    explode = [0, 0.10]\n    colors = sns.color_palette('pastel')[3:5]\n    plt.pie(ratio, colors = colors, autopct='%.0f%%', startangle= 120, explode=explode)\n    plt.legend(['사무가 아니다', '사무이다'], bbox_to_anchor=(1.3, 1))\n    plt.title('\\n\\n 사무판단 비율 \\n')\n    plt.show()\n\n\njudgment_graph(data)\n\n 0의 비율 : 93.03338811360872\n 1의 비율 : 6.966263706674837"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#소관부처명에-따른-사무판단",
    "href": "dev_posts/BaseTable2_EDA.html#소관부처명에-따른-사무판단",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "2) 소관부처명에 따른 사무판단",
    "text": "2) 소관부처명에 따른 사무판단\n\n소관부처명의 종류 : 122개\n사무가 가장 많은 5개 소관부처명 파악\n사무 비율이 가장 큰 5개 소관부처명 파악\n\n\n사무가 가장 많은 소관부처명 파악\n\ndef num_department_judgment_graph(df):\n    # 각 소관부처명 별 사무판단 갯수 파악\n    department_judgment = []\n    for ii in df['소관부처명'].unique():\n        department_count_0 = len(df.loc[(df['소관부처명']==ii) & (df['사무판단']==0)])\n        department_count_1 = len(df.loc[(df['소관부처명']==ii) & (df['사무판단']==1)])\n        # [1인 갯수, 0인 갯수, 1인 비율, 소관부처명]\n        department_judgment.append([department_count_1, department_count_0, department_count_1/(department_count_0+department_count_1), ii])\n    \n    # 사무가 가장 많은 소관부처명 파악\n    department_judgment.sort(key=lambda x:x[0], reverse = True)\n    \n    num_department_judgment = []\n    num_department = []\n    for i in range(len(department_judgment)):\n        num_department_judgment.append(department_judgment[i][0])\n        num_department.append(department_judgment[i][3])\n        \n    # 사무가 많은 소관부처명 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    plt.bar(range(len(num_department_judgment)), num_department_judgment, color=colors)\n    plt.title('\\n\\n\\n 소관부처별 사무판단 개수 파악\\n')\n    plt.ylabel('개수')\n    plt.xlim([-1,len(num_department_judgment)])\n    plt.xticks([])\n    plt.ylim([0,6000]);\n    plt.yticks(np.arange(0, 6000, step=1000));\n    \n    # 사무가 가장 많은 소관부처명 5개 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    bar = plt.bar(range(5), num_department_judgment[:5], color=colors)\n    plt.title('\\n\\n\\n 소관부처별 사무판단 Top5 개수 파악\\n')\n    plt.ylabel('개수')\n    plt.xlim([-0.5,4.5])\n    plt.xticks(np.arange(0, 5, 1), labels = [num_department[0],num_department[1],num_department[2],num_department[3],num_department[4]])\n    plt.ylim([0,6000]);\n    plt.yticks(np.arange(0, 6000, step=1000));\n    for rect in bar:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%d' % height, ha='center', va='bottom', size = 10)\n\n\nnum_department_judgment_graph(data)"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#section-1",
    "href": "dev_posts/BaseTable2_EDA.html#section-1",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "",
    "text": "사무비율이 가장 큰 소관부처명 파악\n\ndef ratio_department_judgment_graph(df):\n    # 각 소관부처명 별 사무판단 갯수 파악\n    department_judgment = []\n    for ii in df['소관부처명'].unique():\n        department_count_0 = len(df.loc[(df['소관부처명']==ii) & (df['사무판단']==0)])\n        department_count_1 = len(df.loc[(df['소관부처명']==ii) & (df['사무판단']==1)])\n        # [1인 갯수, 0인 갯수, 1인 비율, 소관부처명]\n        department_judgment.append([department_count_1, department_count_0, department_count_1/(department_count_0+department_count_1), ii])\n    \n    # 사무비율이 가장 큰 소관부처명 파악\n    department_judgment.sort(key=lambda x:x[2], reverse = True)\n    \n    ratio_department_judgment = []\n    ratio_department = []\n    for i in range(len(department_judgment)):\n        ratio_department_judgment.append(department_judgment[i][2])\n        ratio_department.append(department_judgment[i][3])\n        \n    # 사무가 많은 소관부처명 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    plt.bar(range(len(ratio_department_judgment)), ratio_department_judgment, color=colors)\n    plt.title('\\n\\n\\n 소관부처별 사무판단 비율 파악\\n')\n    plt.ylabel('비율')\n    plt.xlim([-1,len(ratio_department_judgment)])\n    plt.xticks([])\n    plt.ylim([0,0.4]);\n    plt.yticks(np.arange(0, 0.5, step=0.1));\n    \n    # 사무가 가장 많은 소관부처명 5개 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    bar = plt.bar(range(5), ratio_department_judgment[:5], color=colors)\n    plt.title('\\n\\n\\n 소관부처별 사무판단 Top5 비율 파악\\n')\n    plt.ylabel('비율')\n    plt.xlim([-0.5,4.5])\n    plt.xticks(np.arange(0, 5, 1), labels = [ratio_department[0].replace(',','\\n'),ratio_department[1].replace(',','\\n'),ratio_department[2].replace(',','\\n'),ratio_department[3].replace(',','\\n'),ratio_department[4].replace(',','\\n')])\n    plt.ylim([0,0.4]);\n    plt.yticks(np.arange(0, 0.5, step=0.1));\n    for rect in bar:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.3f' % height, ha='center', va='bottom', size = 10)\n\n\nratio_department_judgment_graph(data)"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#법령명에-따른-사무판단",
    "href": "dev_posts/BaseTable2_EDA.html#법령명에-따른-사무판단",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "3) 법령명에 따른 사무판단",
    "text": "3) 법령명에 따른 사무판단\n\n법령명의 종류 : 4324개\n사무가 가장 많은 5개 법령명 파악\n사무 비율이 가장 큰 5개 법령명 파악\n\n\n사무가 가장 많은 법령명 파악\n\ndef num_lawname_judgment_graph(df):\n    # 각 법령명 별 사무판단 갯수 파악\n    lawname_judgment = []\n    for ii in df['법령명'].unique():\n        lawname_count_0 = len(df.loc[(df['법령명']==ii) & (df['사무판단']==0)])\n        lawname_count_1 = len(df.loc[(df['법령명']==ii) & (df['사무판단']==1)])\n        # [1인 갯수, 0인 갯수, 1인 비율, 법령명]\n        lawname_judgment.append([lawname_count_1, lawname_count_0, lawname_count_1/(lawname_count_0+lawname_count_1), ii])\n    \n    # 사무가 가장 많은 법령명 파악\n    lawname_judgment.sort(key=lambda x:x[0], reverse = True)\n    \n    num_lawname_judgment = []\n    num_lawname = []\n    for i in range(len(lawname_judgment)):\n        num_lawname_judgment.append(lawname_judgment[i][0])\n        num_lawname.append(lawname_judgment[i][3])\n        \n    # 사무가 많은 법령명 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    plt.bar(range(len(num_lawname_judgment)), num_lawname_judgment, color=colors)\n    plt.title('\\n\\n\\n 법령명별 사무판단 개수 파악\\n')\n    plt.ylabel('개수')\n    plt.xlim([-1,len(num_lawname_judgment)])\n    plt.xticks([])\n    plt.ylim([0,400]);\n    plt.yticks(np.arange(0, 400, step=100));\n    \n    # 사무가 가장 많은 법령명 5개 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    bar = plt.bar(range(5), num_lawname_judgment[:5], color=colors)\n    plt.title('\\n\\n\\n 법령명별 사무판단 Top5 개수 파악\\n')\n    plt.ylabel('개수')\n    plt.xlim([-0.5,4.5])\n    plt.xticks(np.arange(0, 5, 1), labels = ['\\n'+num_lawname[0],num_lawname[1],'\\n\\n'+num_lawname[2],num_lawname[3],'\\n'+num_lawname[4]])\n    plt.ylim([0,400]);\n    plt.yticks(np.arange(0, 400, step=100));\n    for rect in bar:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%d' % height, ha='center', va='bottom', size = 10)\n\n\nnum_lawname_judgment_graph(data)\n\n['제주특별자치도 설치 및 국제자유도시 조성을 위한 특별법', '자본시장과 금융투자업에 관한 법률', '지방세특례제한법', '재난 및 안전관리 기본법', '대기환경보전법']"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#section-3",
    "href": "dev_posts/BaseTable2_EDA.html#section-3",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "",
    "text": "사무비율이 가장 큰 법령명 파악\n\ndef ratio_lawname_judgment_graph(df):\n    # 각 법령명 별 사무판단 갯수 파악\n    lawname_judgment = []\n    for ii in df['법령명'].unique():\n        lawname_count_0 = len(df.loc[(df['법령명']==ii) & (df['사무판단']==0)])\n        lawname_count_1 = len(df.loc[(df['법령명']==ii) & (df['사무판단']==1)])\n        # [1인 갯수, 0인 갯수, 1인 비율, 법령명]\n        lawname_judgment.append([lawname_count_1, lawname_count_0, lawname_count_1/(lawname_count_0+lawname_count_1), ii])\n    \n    # 사무가 가장 많은 법령명 파악\n    lawname_judgment.sort(key=lambda x:x[2], reverse = True)\n    \n    ratio_lawname_judgment = []\n    ratio_lawname = []\n    for i in range(len(lawname_judgment)):\n        ratio_lawname_judgment.append(lawname_judgment[i][2])\n        ratio_lawname.append(lawname_judgment[i][3])\n\n    # 사무가 많은 법령명 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    plt.bar(range(len(ratio_lawname_judgment)), ratio_lawname_judgment, color=colors)\n    plt.title('\\n\\n\\n 법령명별 사무판단 비율 파악\\n')\n    plt.ylabel('비율')\n    plt.xlim([-1,len(ratio_lawname_judgment)])\n    plt.xticks([])\n    plt.ylim([0,1.2]);\n    plt.yticks(np.arange(0, 1.2, step=0.2));\n    \n    # 사무가 가장 많은 법령명 5개 순서대로 막대 그래프\n    fig, ax = plt.subplots(figsize=(7.5, 4.5))\n    colors = sns.color_palette('pastel')[4]\n    bar = plt.bar(range(5), ratio_lawname_judgment[:5], color=colors)\n    plt.title('\\n\\n\\n 법령명별 사무판단 Top5 비율 파악\\n')\n    plt.ylabel('비율')\n    plt.xlim([-0.5,4.5])\n    plt.xticks(np.arange(0, 5, 1), labels = [ratio_lawname[0],'\\n'+ratio_lawname[1],'\\n\\n'+ratio_lawname[2],ratio_lawname[3],'\\n'+ratio_lawname[4]])\n    plt.ylim([0,1.2]);\n    plt.yticks(np.arange(0, 1.2, step=0.2));\n    for rect in bar:\n        height = rect.get_height()\n        plt.text(rect.get_x() + rect.get_width()/2.0, height, '%.3f' % height, ha='center', va='bottom', size = 10)\n\n\nratio_lawname_judgment_graph(data)"
  },
  {
    "objectID": "dev_posts/BaseTable2_EDA.html#법령구분에-따른-사무판단",
    "href": "dev_posts/BaseTable2_EDA.html#법령구분에-따른-사무판단",
    "title": "EDA ( BaseTable_2.csv 사용 )",
    "section": "4) 법령구분에 따른 사무판단",
    "text": "4) 법령구분에 따른 사무판단\n\n법령구분 종류: 3개\n\n법령구분이 1인 경우 : 365424개\n법령구분이 2인 경우 : 319805개\n법령구분이 3인 경우 : 176395개\n\n\n\ndef lawclass_judgment_graph(df):\n    groups = ['1', '2', '3']\n    values1 = [len(df.loc[(df['법령구분']==1) & (df['사무판단']==0) ]), len(df.loc[(df['법령구분']==2) & (df['사무판단']==0) ]), len(df.loc[(df['법령구분']==3) & (df['사무판단']==0) ])]\n    values2 = [len(df.loc[(df['법령구분']==1) & (df['사무판단']==1) ]), len(df.loc[(df['법령구분']==2) & (df['사무판단']==1) ]), len(df.loc[(df['법령구분']==3) & (df['사무판단']==1) ])]\n    fig, ax = plt.subplots(figsize=(3, 5))\n    colors = sns.color_palette('pastel')[3:5]\n\n    # stack bar 로 구성\n    ax.bar(groups, values1, color = colors[0])\n    ax.bar(groups, values2, bottom = values1, color = colors[1])\n    plt.title('법령구분에 따른 사무판단 여부 파악\\n\\n')\n    plt.xlabel('법령구분')\n    plt.ylabel('개수')\n    plt.legend(['사무가 아니다', '사무이다'], bbox_to_anchor=(1.7, 1))\n    plt.ylim([0,400000])\n    plt.yticks(np.arange(0, 500000, step=100000))\n    plt.show()\n\n\nlawclass_judgment_graph(data)\n\n\n\n\n\n# def law_work_count(i):\n#     a = len(df.loc[(df['법령구분']==i) & (df['사무판단']==0), ])\n#     b = len(df.loc[(df['법령구분']==i) & (df['사무판단']==1), ])\n#     c = len(df.loc[(df['법령구분']==i) & (df['사무판단']==2), ])\n\n#     print(f\"법령 구분 {i}일때 ========\")\n#     print(f\"사무x: {a}\")\n#     print(f\"사무O: {b}\")\n#     print(f\"애매: {c}\")\n    \n\n#     result = [a, b, c]\n#     return result\n\n\n# result = []\n# for i in range(1, 4):\n#     result.append(law_work_count(i))\n# result\n\n\n# # 법령 구분에 따른 사무판단 비율 계산\n# ratio_df = df.groupby(['법령구분', '사무판단']).size().unstack().T\n# ratio_df = ratio_df.div(ratio_df.sum(axis=1), axis=0)\n\n# # 비율을 시각화\n# ratio_df.T.plot(kind='bar', stacked=True)\n# plt.title('법령구분에 따른 사무판단 비율')\n# plt.xlabel('법령구분')\n# plt.ylabel('Administrative Decision')\n# plt.show()"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#조문-조문제목-처리",
    "href": "dev_posts/MakeBaseTable.html#조문-조문제목-처리",
    "title": "Base Table 만들기",
    "section": "5) 조문, 조문제목 처리",
    "text": "5) 조문, 조문제목 처리\n\n조문, 조문 제목 null값이면 ’0’으로 채움\n\n\ndef law_preprocessing(df):\n    df.loc[df['조문제목'].isna(), '조문제목'] = '0'\n    df.loc[df['조문'].isna(), '조문'] = '0'\n    return df\n\n\ndata = law_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#사무판단-처리-및-보류항-제거",
    "href": "dev_posts/MakeBaseTable.html#사무판단-처리-및-보류항-제거",
    "title": "Base Table 만들기",
    "section": "5) 사무판단 처리 및 보류항 제거",
    "text": "5) 사무판단 처리 및 보류항 제거\n\n’ ’ -&gt; nan, ‘0’ -&gt; 0 , ‘1’ -&gt; 1, ‘0 1’ -&gt; 2 float 형태로 변환\n\n\ndef decision_preprocessing(df):\n    # 표기방식 통일\n    idx_nan = df[(df['사무판단'] == ' ')].index #idx_nan: '사무판단'이 nan인 행의 index\n    for i in idx_nan:\n        df.loc[i,'사무판단'] = np.nan\n    idx_0 = df[(df['사무판단'] == '0')].index #idx_0: '사무판단'이 '0'인 행의 index\n    for i in idx_0:\n        df.loc[i,'사무판단'] = 0\n    idx_1 = df[(df['사무판단'] == '1')].index #idx_1: '사무판단'이 '1'인 행의 index\n    for i in idx_1:\n        df.loc[i,'사무판단'] = 1\n    idx_2 = df[(df['사무판단'] == '0 1')].index #idx_2: '사무판단'이 '0 1'인 행의 index\n    for i in idx_2:\n        df.loc[i,'사무판단'] = 2\n\n    # 오류 행 삭제\n    ## 경우1: 사무가 아님에도 사무 유형이 분류된 경우\n    delete_0_idx = list((df[(df['사무판단'] == 0)  & (df['사무유형(소분류)'].notna())]).index)\n    df = df.drop(delete_0_idx, axis = 0)\n    \n    ## 경우2: 사무임에도 사무 유형이 분류되지 않은 경우\n    delete_1_idx1 = list((df[(df['사무판단'] == 1)  & (df['사무유형'].isnull())]).index)\n    df = df.drop(delete_1_idx1, axis = 0)\n    \n    delete_1_idx2 = list((df[(df['사무판단'] == 1)  & (df['사무유형(소분류)'].isnull())]).index)\n    df = df.drop(delete_1_idx2, axis = 0)\n\n    # 결측행 처리\n    ## 경우1: 사무 유형이 분류된 경우 =&gt; '1'로 채움\n    change_1_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].notna()) & (df['사무유형(소분류)'].notna())].index\n    df.loc[change_1_idx, '사무판단'] = 1\n    \n    # 경우2: 사무 유형이 분류되지 않은 경우 =&gt; '0'으로 채움\n    change_0_idx = df[(df['사무판단'].isnull())  & (df['사무유형'].isnull()) & (df['사무유형(소분류)'].isnull())].index\n    df.loc[change_0_idx,'사무판단'] = 0\n    \n    # 사무판단 보류 행 삭제 \n    delete_index = df[df['사무판단'] == 2].index\n    delete_index = delete_index.sort_values(ascending = False).tolist()\n    for i in delete_index:\n        df.drop(i, inplace=True)\n    \n    # 자료형 통일\n    df['사무판단'] = df['사무판단'].astype('int64')\n\n    return df\n\n\ndata = decision_preprocessing(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#사무판단-근거-처리",
    "href": "dev_posts/MakeBaseTable.html#사무판단-근거-처리",
    "title": "Base Table 만들기",
    "section": "6) 사무판단 근거 처리",
    "text": "6) 사무판단 근거 처리\n\n사무판단 공백처리, 사무판단 근거: (삭제, 판단 불가) -&gt; np.nan 변환\n\n\n# 열 안의 공백 제거 함수 정의\ndef remove_whitespace(cell):\n    return cell.strip() if isinstance(cell, str) else cell\n\ndata['사무판단근거'] = data['사무판단근거'].apply(remove_whitespace)\n\n\ndef change_null(df):\n    df.loc[df['사무판단근거']=='판단 불가', '사무판단근거'] = np.nan\n    df.loc[df['사무판단근거']=='판단불가', '사무판단근거'] = np.nan\n    df.loc[df['사무판단근거']=='삭제', '사무판단근거'] = np.nan\n    return df\n\n\ndata = change_null(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#사무유형-처리",
    "href": "dev_posts/MakeBaseTable.html#사무유형-처리",
    "title": "Base Table 만들기",
    "section": "7) 사무유형 처리",
    "text": "7) 사무유형 처리\n\n사무유형 공백 제거, 사무유형에 없는 유형(공동, 정부) 수정\n\n\ndef change_law_type(df):\n    df.loc[df['사무유형']=='국가 ', '사무유형'] = '국가'\n    df.loc[df['사무유형']=='시도 ', '사무유형'] = '시도'\n    df.loc[df['사무유형']=='시군구 ', '사무유형'] = '시군구'\n    df.loc[df['사무유형']=='국가\\n시도', '사무유형'] = '국가-시도'\n    df.loc[df['사무유형']=='국가 ', '사무유형'] = '국가'\n    df.loc[df['사무유형']=='\\n국가-시도-시군구', '사무유형'] = '국가-시도-시군구'\n\n    df.loc[df['사무유형']=='공동', '사무유형'] = '국가-시도'\n    df.loc[df['사무유형']=='정부', '사무유형'] = '국가' \n    return df\n\n\ndata = change_law_type(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#위임-사무-판단-처리",
    "href": "dev_posts/MakeBaseTable.html#위임-사무-판단-처리",
    "title": "Base Table 만들기",
    "section": "8) 위임 사무 판단 처리",
    "text": "8) 위임 사무 판단 처리\n\n자료형 변경 및 이상치 변경\n\n\ndef change_delegated_tasks(df):\n    df.loc[(df['위임사무판단']==0.0)|(df['위임사무판단']=='0')|(df['위임사무판단']=='0.0')|(df['위임사무판단']==0.0)|(df['위임사무판단'].isna()), '위임사무판단'] = 0\n    df.loc[(df['위임사무판단']==1.0)|(df['위임사무판단']=='1'), '위임사무판단'] = 1\n    df.loc[(df['위임사무판단']=='0 1'), '위임사무판단'] = 2    # 애매한 친구\n\n    df['위임사무판단'] = df['위임사무판단'].astype(int)\n    return df\n\n\ndata = change_delegated_tasks(data)"
  },
  {
    "objectID": "dev_posts/MakeBaseTable.html#나머지-변수들-자료형-통일",
    "href": "dev_posts/MakeBaseTable.html#나머지-변수들-자료형-통일",
    "title": "Base Table 만들기",
    "section": "9) 나머지 변수들 자료형 통일",
    "text": "9) 나머지 변수들 자료형 통일\n\n특행기관, 재위임 사무판단, 재위임 근거 규정, 재수임기관, 위탁사무판단\n\n\ndef change_else(df):\n    # 특행기관\n    df.loc[(df['특행기관']=='-')|(df['특행기관']=='0')|(df['특행기관']=='0.0')|(df['특행기관'].isna())|(df['특행기관']=='`')|(df['특행기관']=='시-도 본부장'), '특행기관'] = 0\n    df.loc[(df['특행기관']==1.0)|(df['특행기관']=='1'), '특행기관'] = 1\n    df['특행기관'] = df['특행기관'].astype(int)\n\n    # 재위임 사무판단\n    df.loc[(df['재위임사무판단']=='국가-시도공동사무')|(df['재위임사무판단']=='0')|(df['재위임사무판단']=='0.0')|(df['재위임사무판단'].isna())|(df['재위임사무판단']==0.0), '재위임사무판단'] = 0\n    df.loc[(df['재위임사무판단']==1.0)|(df['재위임사무판단']=='1'), '재위임사무판단'] = 1\n    df['재위임사무판단'] = df['재위임사무판단'].astype(int)\n\n    # 재위임 근거 규정\n    df.loc[(df['재위임근거규정']==0.0)|(df['재위임근거규정'].isna()), '재위임근거규정'] = np.nan\n\n    # 재수임기관\n    df.loc[(df['재수임기관']==0.0)|(df['재수임기관'].isna()), '재수임기관'] = np.nan\n\n    # 위탁사무판단\n    df.loc[(df['위탁사무판단']==0.)|(df['위탁사무판단'].isna()), '위탁사무판단'] = 0\n    df.loc[(df['위탁사무판단']==1.), '위탁사무판단'] = 1\n    df['위탁사무판단'] = df['위탁사무판단'].astype(int)\n    \n    return df\n\n\ndata = change_else(data)"
  },
  {
    "objectID": "dev_posts/DL정리.html",
    "href": "dev_posts/DL정리.html",
    "title": "딥러닝 코드 정리",
    "section": "",
    "text": "# from google.colab import drive\n# drive.mount('/content/drive')\n# cd /content/drive/MyDrive/Colab Notebooks/법령프로젝트\nimport os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'\nimport pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm"
  },
  {
    "objectID": "dev_posts/DL정리.html#조문-사무판단-추출",
    "href": "dev_posts/DL정리.html#조문-사무판단-추출",
    "title": "딥러닝 코드 정리",
    "section": "1) [조문, 사무판단] 추출",
    "text": "1) [조문, 사무판단] 추출\n\nsub_df = df.loc[df['사무판단']!=2, [\"조문\", \"사무판단\"]]\nsub_df\n\n\n\n\n\n\n\n\n조문\n사무판단\n\n\n\n\n0\n제1장 총칙\n0\n\n\n1\n제1조(목적) 이 법은 개인정보의 처리 및 보호에 관한 사항을 정함으로써 개인의 자...\n0\n\n\n2\n제2조(정의) 이 법에서 사용하는 용어의 뜻은 다음과 같다. &lt;개정 2014.3.2...\n0\n\n\n3\n1. \"개인정보\"란 살아 있는 개인에 관한 정보로서 다음 각 목의 어느 하나에 해당...\n0\n\n\n4\n1의2. \"가명처리\"란 개인정보의 일부를 삭제하거나 일부 또는 전부를 대체하는 등의...\n0\n\n\n...\n...\n...\n\n\n861616\n1. 한국수자원공사\n0\n\n\n861617\n2. 법 제56조에 따른 한국상하수도협회\n0\n\n\n861618\n제32조(규제의 재검토) 환경부장관은 다음 각 호의 사항에 대하여 다음 각 호의 기...\n0\n\n\n861619\n1. 제23조의2제1항ㆍ제4항 및 별표 7의2에 따른 저수조청소업의 인력ㆍ시설 및 ...\n0\n\n\n861620\n2. 제31조 및 별표 8에 따른 기술진단 대행 기관의 장비와 기술인력: 2014년...\n0\n\n\n\n\n861621 rows × 2 columns\n\n\n\n\n#sub_df.to_csv('sub_df.csv')      # 임시 저장"
  },
  {
    "objectID": "dev_posts/DL정리.html#모델-정의",
    "href": "dev_posts/DL정리.html#모델-정의",
    "title": "딥러닝 코드 정리",
    "section": "1) 모델 정의",
    "text": "1) 모델 정의\n\nimport torch.nn as nn\n\nclass MyModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(MyModel, self).__init__()\n        self.layer1 = nn.Linear(input_size, hidden_size)\n        self.batch_norm = nn.BatchNorm1d(hidden_size)  \n        self.relu = nn.ReLU()\n        self.layer2 = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        # 모델의 forward 계산 로직\n        x = self.layer1(input_ids.float())\n        x = self.batch_norm(x)\n        x = self.relu(x)\n        x = self.layer2(x)\n\n        return x\n\n\n# 모델 및 토크나이저 불러오기\nmodel_name = 'klue/roberta-large'\n#model = AutoModelForSequenceClassification.from_pretrained(model_name)   # gpu부족으로 제한\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ninput_size = 512\nhidden_size = 128\noutput_size = 2\n\nmodel = MyModel(input_size, hidden_size, output_size)"
  },
  {
    "objectID": "dev_posts/DL정리.html#입력-데이터셋-정리",
    "href": "dev_posts/DL정리.html#입력-데이터셋-정리",
    "title": "딥러닝 코드 정리",
    "section": "2) 입력 데이터셋 정리",
    "text": "2) 입력 데이터셋 정리\n\n512 차원으로 padding 진행\nbatch size = 16\n\n\n# 데이터셋 정의\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        text = self.data.iloc[idx]['조문']\n        label = torch.tensor(self.data.iloc[idx]['사무판단'], dtype=torch.long)\n        tokenized_data = tokenizer(text, padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n        return {'input_ids': tokenized_data['input_ids'].squeeze(),\n                'attention_mask': tokenized_data['attention_mask'].squeeze(),\n                'labels': label}\n\n\n# 각 배치의 텍스트 길이를 맞추기\ndef collate_fn(batch):\n    input_ids = [item['input_ids'] for item in batch]\n    attention_mask = [item['attention_mask'] for item in batch]\n    labels = [item['labels'] for item in batch]\n\n    # 패딩\n    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n\n    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': torch.stack(labels)}\n\n\n# 데이터로더 생성\nbatch_size = 16  # 배치크기 조절\n\ntrain_dataset = CustomDataset(train_df, tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\nval_dataset = CustomDataset(val_df, tokenizer)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n\nval_dataset\n\n&lt;__main__.CustomDataset at 0x2ca5ee6b0&gt;\n\n\n\n# 모델을 GPU로 이동  -&gt; 작은 신경망이라 cpu로 돌림\n#model.to(device)"
  },
  {
    "objectID": "dev_posts/DL정리.html#튜닝-파라미터-지정",
    "href": "dev_posts/DL정리.html#튜닝-파라미터-지정",
    "title": "딥러닝 코드 정리",
    "section": "3) 튜닝 파라미터 지정",
    "text": "3) 튜닝 파라미터 지정\n\noptimizer: Adam\nlearning rate: 1e-5\n\n\n# 나머지 파라미터 정리\noptimizer = AdamW(model.parameters(), lr=1e-5)\nnum_epochs = 5\ntotal_steps = len(train_loader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n\n/Users/ihongju/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn("
  },
  {
    "objectID": "dev_posts/DL정리.html#모델-학습",
    "href": "dev_posts/DL정리.html#모델-학습",
    "title": "딥러닝 코드 정리",
    "section": "4) 모델 학습",
    "text": "4) 모델 학습\n\n# 손실 기록을 위한 리스트\ntrain_losses = []\nval_losses = []\n\n\nfrom sklearn.metrics import recall_score\n\n# 클래스 0에 대한 적절한 가중치를 계산 (imbalance data 고려)\nclass_0_weight = 0.06\nclass_1_weight = 1.0\n\nclass_weights = [class_0_weight, class_1_weight]    # 0, 1의 가중치를 차등 부여\n\n# 학습 루프에서 'labels'를 사용하는 부분 수정\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training'):\n        inputs = {key: value.to(device) for key, value in batch.items() if key != 'labels'}\n        labels = batch['labels'].to(device)\n\n        # 모델에 토큰화된 입력 데이터 전달\n        logits = model(**inputs)\n        if logits.dtype != torch.float32:\n            logits = logits.float()\n        loss = torch.nn.functional.cross_entropy(logits, labels, weight=torch.Tensor(class_weights).to(device))\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n\n    avg_train_loss = total_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Average Training Loss: {avg_train_loss}')\n\n    model.eval()\n    val_loss = 0.0\n    y_true = []  # 실제 레이블을 저장할 리스트\n    y_pred = []  # 모델의 예측 결과를 저장할 리스트\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Validation'):\n            inputs = {key: value.to(device) for key, value in batch.items() if key != 'labels'}\n            labels = batch['labels'].to(device)\n\n            # 모델에 토큰화된 입력 데이터 전달\n            outputs = model(**inputs)\n            if outputs.dtype != torch.float32:\n                outputs = outputs.float()\n\n            _, predictions = torch.max(outputs, 1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(predictions.cpu().numpy())\n            val_loss += torch.nn.functional.cross_entropy(outputs, labels, weight=torch.Tensor(class_weights).to(device))\n\n    avg_val_loss = val_loss / len(val_loader)\n    val_losses.append(avg_val_loss)\n    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {avg_val_loss}')\n    # Calculate Recall\n    recall = recall_score(y_true, y_pred, average='weighted')  # 'weighted'는 각 클래스의 샘플 수에 따라 가중 평균을 계산합니다.\n    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Recall: {recall}')\n\nEpoch 1/5 - Training: 100%|██████████████| 43081/43081 [03:14&lt;00:00, 221.63it/s]\nEpoch 1/5 - Validation: 100%|████████████| 10771/10771 [00:40&lt;00:00, 264.66it/s]\nEpoch 2/5 - Training: 100%|██████████████| 43081/43081 [03:13&lt;00:00, 222.08it/s]\nEpoch 2/5 - Validation: 100%|████████████| 10771/10771 [00:40&lt;00:00, 268.15it/s]\nEpoch 3/5 - Training: 100%|██████████████| 43081/43081 [03:11&lt;00:00, 224.61it/s]\nEpoch 3/5 - Validation: 100%|████████████| 10771/10771 [00:40&lt;00:00, 264.87it/s]\nEpoch 4/5 - Training: 100%|██████████████| 43081/43081 [03:12&lt;00:00, 223.36it/s]\nEpoch 4/5 - Validation: 100%|████████████| 10771/10771 [00:41&lt;00:00, 257.78it/s]\nEpoch 5/5 - Training: 100%|██████████████| 43081/43081 [03:12&lt;00:00, 223.52it/s]\nEpoch 5/5 - Validation: 100%|████████████| 10771/10771 [00:40&lt;00:00, 266.81it/s]\n\n\nEpoch 1/5, Average Training Loss: 0.5239665991301568\nEpoch 1/5, Validation Loss: 0.4939045310020447\nEpoch 1/5, Validation Recall: 0.7724125924851298\nEpoch 2/5, Average Training Loss: 0.4985668318130958\nEpoch 2/5, Validation Loss: 0.48241615295410156\nEpoch 2/5, Validation Recall: 0.7729754823734223\nEpoch 3/5, Average Training Loss: 0.4890418174231894\nEpoch 3/5, Validation Loss: 0.4739820957183838\nEpoch 3/5, Validation Recall: 0.7625475119686639\nEpoch 4/5, Average Training Loss: 0.4844063688010349\nEpoch 4/5, Validation Loss: 0.47453540563583374\nEpoch 4/5, Validation Recall: 0.7842100681851153\nEpoch 5/5, Average Training Loss: 0.48282361713480765\nEpoch 5/5, Validation Loss: 0.46963998675346375\nEpoch 5/5, Validation Recall: 0.7725982881183809"
  },
  {
    "objectID": "dev_posts/DL정리.html#loss-그래프",
    "href": "dev_posts/DL정리.html#loss-그래프",
    "title": "딥러닝 코드 정리",
    "section": "1) loss 그래프",
    "text": "1) loss 그래프\n\n# 손실 값 시각화\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "dev_posts/DL정리.html#결과-확인",
    "href": "dev_posts/DL정리.html#결과-확인",
    "title": "딥러닝 코드 정리",
    "section": "1) 결과 확인",
    "text": "1) 결과 확인\n\nprob = [sublist[1] for sublist in all_probs]\n\n\n# 결과 데이터프레임 생성\nresult_df = pd.DataFrame({\n    'True_Labels': all_labels,\n    'Predictions': all_preds,\n    'Probability_1': prob\n})\n\nresult_df\n\n\n\n\n\n\n\n\nTrue_Labels\nPredictions\nProbability_1\n\n\n\n\n0\n0\n0\n0.026727\n\n\n1\n0\n0\n0.097768\n\n\n2\n0\n0\n0.036723\n\n\n3\n0\n0\n0.353132\n\n\n4\n0\n1\n0.520629\n\n\n...\n...\n...\n...\n\n\n172320\n0\n0\n0.008365\n\n\n172321\n0\n1\n0.508366\n\n\n172322\n0\n0\n0.012416\n\n\n172323\n1\n1\n0.578635\n\n\n172324\n0\n1\n0.726729\n\n\n\n\n172325 rows × 3 columns\n\n\n\n\n#result_df.to_csv('epo5_result_df.csv')   # 결과 저장\n\n\n#torch.save(model.state_dict(), \"model_06_1_5.pth\")    # 모델 저장"
  },
  {
    "objectID": "dev_posts/DL정리.html#결과-통계량-확인",
    "href": "dev_posts/DL정리.html#결과-통계량-확인",
    "title": "딥러닝 코드 정리",
    "section": "2) 결과 통계량 확인",
    "text": "2) 결과 통계량 확인\n\nresult_df.loc[result_df['Predictions']==0, ].describe()\n\n\n\n\n\n\n\n\nTrue_Labels\nPredictions\nProbability_1\n\n\n\n\ncount\n127067.000000\n127067.0\n127067.000000\n\n\nmean\n0.023350\n0.0\n0.152123\n\n\nstd\n0.151013\n0.0\n0.154932\n\n\nmin\n0.000000\n0.0\n0.002187\n\n\n25%\n0.000000\n0.0\n0.023082\n\n\n50%\n0.000000\n0.0\n0.070822\n\n\n75%\n0.000000\n0.0\n0.275795\n\n\nmax\n1.000000\n0.0\n0.499992\n\n\n\n\n\n\n\n\nresult_df.loc[result_df['Predictions']==1, ].describe()\n\n\n\n\n\n\n\n\nTrue_Labels\nPredictions\nProbability_1\n\n\n\n\ncount\n45258.000000\n45258.0\n45258.000000\n\n\nmean\n0.199700\n1.0\n0.653751\n\n\nstd\n0.399779\n0.0\n0.096457\n\n\nmin\n0.000000\n1.0\n0.500006\n\n\n25%\n0.000000\n1.0\n0.573960\n\n\n50%\n0.000000\n1.0\n0.645236\n\n\n75%\n0.000000\n1.0\n0.723073\n\n\nmax\n1.000000\n1.0\n0.965777"
  },
  {
    "objectID": "dev_posts/DL정리.html#roc-커브-확인",
    "href": "dev_posts/DL정리.html#roc-커브-확인",
    "title": "딥러닝 코드 정리",
    "section": "3) ROC 커브 확인",
    "text": "3) ROC 커브 확인\n\nfrom sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thresholds = roc_curve(result_df['True_Labels'], result_df['Probability_1'])\nroc_auc = auc(fpr, tpr)\n\n# ROC 커브 그리기\nplt.figure(figsize=(8, 8))\nplt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC) Curve')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n디폴트 threshold 기준 평가 지표 확인\n\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\n# confusion matrix, accuracy, precision, recall, f1-score 계산\nconf_matrix = confusion_matrix(all_labels, all_preds)\naccuracy = accuracy_score(all_labels, all_preds)\nprecision = precision_score(all_labels, all_preds)\nrecall = recall_score(all_labels, all_preds)\nf1 = f1_score(all_labels, all_preds)\n\n# 디폴트 결과 출력\nprint(f\"Avg Validation Loss: {avg_val_loss}\")\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1-Score: {f1}\")\nprint(\"Confusion Matrix:\")\nprint(conf_matrix)\n\nAvg Validation Loss: 0.39846527576446533\nAccuracy: 0.7725982881183809\nPrecision: 0.1996995006407707\nRecall: 0.7528529779258643\nF1-Score: 0.31566631157990327\nConfusion Matrix:\n[[124100  36220]\n [  2967   9038]]\n\n\n\nimport seaborn as sns\n\n# seaborn을 사용하여 heatmap으로 confusion matrix 시각화\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\nplt.title('Confusion Matrix Neural Network')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()"
  },
  {
    "objectID": "dev_posts/DL정리.html#threshold-조절",
    "href": "dev_posts/DL정리.html#threshold-조절",
    "title": "딥러닝 코드 정리",
    "section": "4) threshold 조절",
    "text": "4) threshold 조절\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef cal_result(preds, y):\n    conf_matrix = confusion_matrix(y, preds)\n    accuracy = accuracy_score(y, preds)\n    precision = precision_score(y, preds)\n    recall = recall_score(y, preds)\n    f1 = f1_score(y, preds)\n    \n    return accuracy, recall\n\n\nx = np.arange(0.15, 0.29, 0.01).tolist()\ny_acc = []\ny_rec = []\n\nfor i in x:\n    threshold = i\n    result_df['new_predictions'] = 0\n    result_df.loc[result_df['Probability_1']&gt;threshold, 'new_predictions'] = 1\n    acc, rec = cal_result(result_df['new_predictions'], result_df['True_Labels'])\n    y_acc.append(acc)\n    y_rec.append(rec)\n\n\nthreshold에 따른 accuracy, recall 그래프 확인\n\n\nimport matplotlib.pyplot as plt\n\n# accuracy 그래프\nplt.plot(x, y_acc, label='accuracy', color='blue')\n\n# recall 그래프\nplt.plot(x, y_rec, label='recall', color='red')\n\n# 그래프에 제목과 축 레이블 추가\nplt.title('Result Line Graph')\nplt.xlabel('Threshold')\nplt.ylabel('Value')\n\n# 범례 추가\nplt.legend()\n\n# 그래프 표시\nplt.show()\n\n\n\n\n\nprint(f\"accuracy 최대값: {y_acc[-1]}, 최소값: {y_acc[0]}\")\nprint(f\"recall 최소값: {y_rec[-1]}, 최대값: {y_rec[0]}\")\n\naccuracy 최대값: 0.6184186856230959, 최소값: 0.5187291455099377\nrecall 최소값: 0.9450229071220325, 최대값: 0.9790920449812578\n\n\n\nprediction 중위수, 3분위수에 따른 결과 모두 출력\n\n\ndef cal_result2(preds, y):\n    conf_matrix = confusion_matrix(y, preds)\n    accuracy = accuracy_score(y, preds)\n    precision = precision_score(y, preds)\n    recall = recall_score(y, preds)\n    f1 = f1_score(y, preds)\n    \n    # 결과 출력\n    print(f\"Avg Validation Loss: {avg_val_loss}\")\n    print(f\"Accuracy: {accuracy}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"F1-Score: {f1}\")\n    print(\"Confusion Matrix:\")\n    print(conf_matrix)\n    \n    return conf_matrix\n\n# prediction 중위수\nprint('======threshold = 0.16======')\nthreshold = 0.16\nresult_df['new_predictions'] = 0\nresult_df.loc[result_df['Probability_1']&gt;threshold, 'new_predictions'] = 1\na = cal_result2(result_df['new_predictions'], result_df['True_Labels'])\n\n# prediction 3분위수\nprint('======threshold = 0.28======')\nthreshold = 0.28\nresult_df['new_predictions'] = 0\nresult_df.loc[result_df['Probability_1']&gt;threshold, 'new_predictions'] = 1\nb = cal_result2(result_df['new_predictions'], result_df['True_Labels'])\n\n======threshold = 0.16======\nAvg Validation Loss: 0.39846527576446533\nAccuracy: 0.5263890903815465\nPrecision: 0.1260341678306651\nRecall: 0.9770928779675135\nF1-Score: 0.22326909350463958\nConfusion Matrix:\n[[78980 81340]\n [  275 11730]]\n======threshold = 0.28======\nAvg Validation Loss: 0.39846527576446533\nAccuracy: 0.6184186856230959\nPrecision: 0.14841511754163342\nRecall: 0.9450229071220325\nF1-Score: 0.25654071410804336\nConfusion Matrix:\n[[95224 65096]\n [  660 11345]]\n\n\n\n# seaborn을 사용하여 heatmap으로 confusion matrix 시각화\nplt.figure(figsize=(8, 6))\nsns.heatmap(a, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\nplt.title('Confusion Matrix Neural Network (0.16)')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(b, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\nplt.title('Confusion Matrix Neural Network (0.28)')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()"
  },
  {
    "objectID": "dev_posts/앙상블.html",
    "href": "dev_posts/앙상블.html",
    "title": "앙상블 코드 정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "dev_posts/앙상블.html#accuracy-recall-11-비율-최적의-threshold-확인",
    "href": "dev_posts/앙상블.html#accuracy-recall-11-비율-최적의-threshold-확인",
    "title": "앙상블 코드 정리",
    "section": "accuracy, recall 1:1 비율 최적의 threshold 확인",
    "text": "accuracy, recall 1:1 비율 최적의 threshold 확인\n\nrf_result = np.sum([rf_y_acc, rf_y_rec], axis=0)\ndeep_result = np.sum([deep_y_acc, deep_y_rec], axis=0)\nlgbm_result = np.sum([lgbm_y_acc, lgbm_y_rec], axis=0)\navg_result = np.sum([avg_y_acc, avg_y_rec], axis=0)\n\nprint(\"====== accuracy, recall 1:1 비율 최적의 threshold =========\")\nprint(\"위치: \",np.argmax(rf_result), np.argmax(deep_result), np.argmax(lgbm_result), np.argmax(avg_result))\nprint(\"임계치값: \", x[2], x[36], x[5], x[25])\nprint(\"최대값: \", rf_result[2], deep_result[36], lgbm_result[5], avg_result[25])\n\n====== accuracy, recall 1:1 비율 최적의 threshold =========\n위치:  2 36 5 25\n임계치값:  0.03 0.37 0.06 0.26\n최대값:  1.6958764956789065 1.5833042533356156 1.71489993923297 1.7053350552169104\n\n\n\nprint('====== rf threshold = 0.03======')\nthreshold = 0.03\nresult['predict'] = 0\nresult.loc[result['rf']&gt;threshold, 'predict'] = 1\na = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== deep threshold = 0.37======')\nthreshold = 0.37\nresult['predict'] = 0\nresult.loc[result['deep']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== lgbm threshold = 0.06======')\nthreshold = 0.06\nresult['predict'] = 0\nresult.loc[result['lgbm']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== avg threshold = 0.02======')\nthreshold = 0.26\nresult['predict'] = 0\nresult.loc[result['avg']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\n====== rf threshold = 0.03======\nAccuracy: 0.8054141883069781\nPrecision: 0.24914349639918895\nRecall: 0.8904623073719283\nF1-Score: 0.38935023310023315\nConfusion Matrix:\n[[128103  32217]\n [  1315  10690]]\n====== deep threshold = 0.37======\nAccuracy: 0.6770985057304512\nPrecision: 0.16635319662981482\nRecall: 0.9062057476051645\nF1-Score: 0.28110384744580247\nConfusion Matrix:\n[[105802  54518]\n [  1126  10879]]\n====== lgbm threshold = 0.06======\nAccuracy: 0.8288524590163935\nPrecision: 0.2744182446726175\nRecall: 0.8860474802165764\nF1-Score: 0.4190517462130912\nConfusion Matrix:\n[[132195  28125]\n [  1368  10637]]\n====== avg threshold = 0.02======\nAccuracy: 0.8637690410561439\nPrecision: 0.3189380307478612\nRecall: 0.8415660141607664\nF1-Score: 0.46257039512842824\nConfusion Matrix:\n[[138746  21574]\n [  1902  10103]]"
  },
  {
    "objectID": "dev_posts/앙상블.html#accuracy-recall-12-비율-최적의-threshold-확인",
    "href": "dev_posts/앙상블.html#accuracy-recall-12-비율-최적의-threshold-확인",
    "title": "앙상블 코드 정리",
    "section": "accuracy, recall 1:2 비율 최적의 threshold 확인",
    "text": "accuracy, recall 1:2 비율 최적의 threshold 확인\n\nrf_result2 = np.sum([rf_y_acc, np.array(rf_y_rec)*2], axis=0)\ndeep_result2 = np.sum([deep_y_acc, np.array(deep_y_rec)*2], axis=0)\nlgbm_result2 = np.sum([lgbm_y_acc, np.array(lgbm_y_rec)*2], axis=0)\navg_result2 = np.sum([avg_y_acc, np.array(avg_y_rec)*2], axis=0)\n\nprint(\"====== accuracy, recall 1:2 비율 최적의 threshold =========\")\nprint(\"위치: \",np.argmax(rf_result2), np.argmax(deep_result2), np.argmax(lgbm_result2), np.argmax(avg_result2))\nprint(\"임계치값: \", x[0], x[30], x[2], x[16])\nprint(\"최대값: \", rf_result2[0], deep_result2[30], lgbm_result2[2], avg_result2[16])\n\n====== accuracy, recall 1:2 비율 최적의 threshold =========\n위치:  0 30 2 16\n임계치값:  0.01 0.31 0.03 0.17\n최대값:  2.6070843662328667 2.5091515969124765 2.6338171296076704 2.61807193711842\n\n\n\nprint('====== rf threshold = 0.01======')\nthreshold = 0.01\nresult['predict'] = 0\nresult.loc[result['rf']&gt;threshold, 'predict'] = 1\na = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== deep threshold = 0.31======')\nthreshold = 0.31\nresult['predict'] = 0\nresult.loc[result['deep']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== lgbm threshold = 0.03======')\nthreshold = 0.03\nresult['predict'] = 0\nresult.loc[result['lgbm']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\nprint('====== avg threshold = 0.17======')\nthreshold = 0.17\nresult['predict'] = 0\nresult.loc[result['avg']&gt;threshold, 'predict'] = 1\nb = cal_result2(result['predict'], result['True_Labels'])\n\n====== rf threshold = 0.01======\nAccuracy: 0.7551893225010881\nPrecision: 0.21208074178654557\nRecall: 0.9259475218658892\nF1-Score: 0.3451155714928825\nConfusion Matrix:\n[[119022  41298]\n [   889  11116]]\n====== deep threshold = 0.31======\nAccuracy: 0.6359321050340926\nPrecision: 0.15356250256074078\nRecall: 0.936609745939192\nF1-Score: 0.2638631403562293\nConfusion Matrix:\n[[98343 61977]\n [  761 11244]]\n====== lgbm threshold = 0.03======\nAccuracy: 0.7514347889162919\nPrecision: 0.21148482976772043\nRecall: 0.9411911703456893\nF1-Score: 0.34536618168480254\nConfusion Matrix:\n[[118192  42128]\n [   706  11299]]\n====== avg threshold = 0.17======\nAccuracy: 0.7223618163354127\nPrecision: 0.19419082888202469\nRecall: 0.9478550603915036\nF1-Score: 0.32234214328205996\nConfusion Matrix:\n[[113102  47218]\n [   626  11379]]"
  }
]